<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Introduction When dealing with large-scale, complex optimization problems or training neural networks, we often encounter situations where programs run for extended periods or fail to complete. Howeve">
<meta property="og:type" content="article">
<meta property="og:title" content="Accelerating Your Program Through Coding Skills">
<meta property="og:url" content="https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/index.html">
<meta property="og:site_name" content="Try to Understand">
<meta property="og:description" content="Introduction When dealing with large-scale, complex optimization problems or training neural networks, we often encounter situations where programs run for extended periods or fail to complete. Howeve">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-03-06T00:00:00.000Z">
<meta property="article:modified_time" content="2025-08-27T21:41:30.771Z">
<meta property="article:author" content="BigdogManLuo">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>Accelerating Your Program Through Coding Skills</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 7.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2023/11/26/Arts-time_travel/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2022/10/02/thoughts-learning_ability/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&text=Accelerating Your Program Through Coding Skills"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&title=Accelerating Your Program Through Coding Skills"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&is_video=false&description=Accelerating Your Program Through Coding Skills"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Accelerating Your Program Through Coding Skills&body=Check out this article: https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&title=Accelerating Your Program Through Coding Skills"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&title=Accelerating Your Program Through Coding Skills"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&title=Accelerating Your Program Through Coding Skills"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&title=Accelerating Your Program Through Coding Skills"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&name=Accelerating Your Program Through Coding Skills&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&t=Accelerating Your Program Through Coding Skills"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#code-optimization"><span class="toc-number">2.</span> <span class="toc-text">Code Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#parallelization"><span class="toc-number">2.1.</span> <span class="toc-text">1 Parallelization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cpu-multi-process-operations"><span class="toc-number">2.1.1.</span> <span class="toc-text">1.1 CPU Multi-process
Operations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpu"><span class="toc-number">2.1.2.</span> <span class="toc-text">1.2 GPU</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#python-implementation-of-gpu-parallel-computing"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">1.2.1 Python
Implementation of GPU Parallel Computing</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#using-gpu-to-train-neural-networks"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">1.2.2 Using GPU to Train
Neural Networks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#precautions"><span class="toc-number">2.1.2.3.</span> <span class="toc-text">1.2.3 Precautions</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#compiler-acceleration"><span class="toc-number">2.2.</span> <span class="toc-text">2 Compiler Acceleration</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#principles"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.1 Principles</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#python-acceleration-solution-based-on-numba"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2 Python
Acceleration Solution Based on Numba</span></a></li></ol></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        Accelerating Your Program Through Coding Skills
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">BigdogManLuo</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2023-03-06T00:00:00.000Z" class="dt-published" itemprop="datePublished">2023-03-06</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/Articles/">Articles</a>
    </div>


      

    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h1 id="introduction">Introduction</h1>
<p>When dealing with large-scale, complex optimization problems or
training neural networks, we often encounter situations where programs
run for extended periods or fail to complete. However, this is not
necessarily due to the large problem scale or limitations of computer
hardware capabilities. Even when attempting to use higher-performance
servers or computers, there’s no guarantee of effectively accelerating
code execution. This is because high-performance hardware typically
needs to be matched with code designed for high-performance
computing.</p>
<p>This article aims to provide some code-level optimization strategies
for program acceleration. By optimizing code structure and designing
high-performance computing solutions, we can effectively accelerate
program execution and improve runtime efficiency. It’s important to note
that this article only covers code-level acceleration solutions and does
not include optimization measures related to algorithms, hardware, etc.
The article is written based on personal experience, and if there are
any shortcomings, please point them out.</p>
<h1 id="code-optimization">Code Optimization</h1>
<p>Simply put, there are mainly two approaches to implementing program
optimization. One is to parallelize tasks and write parallel code to
leverage the parallel computing capabilities of multi-core CPUs or GPUs
to accelerate program execution. The other is to utilize compiler code
optimization mechanisms to compile parts of code that require
interpreters (such as Python, MATLAB) into machine code, achieving
faster program execution speeds.</p>
<h2 id="parallelization">1 Parallelization</h2>
<p>Parallelization requires coordination between software and hardware,
but the prerequisite is that the overall task can be decomposed into
subtasks that can be executed simultaneously. There are two ways to
achieve parallelization: one relies on multi-core CPUs to implement
multi-process operations, and the other relies on GPUs. We will
introduce the implementation methods of these two approaches below.</p>
<h3 id="cpu-multi-process-operations">1.1 CPU Multi-process
Operations</h3>
<p>Multi-process operations leverage the multi-core characteristics of
CPUs to achieve parallel computing. In multi-process operations,
programs are decomposed into multiple subtasks, each running in an
independent process. These processes can execute different tasks in
parallel, thereby accelerating program execution. Multi-process
operations can be implemented using Python’s multiprocessing
library.</p>
<p>When using the multiprocessing library for multi-process operations,
it’s first necessary to decompose tasks into multiple subtasks and
assign each subtask to different processes for execution.</p>
<p>The following is a code framework for the multiprocessing library:
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">worker</span>(<span class="params">num</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Worker <span class="subst">&#123;num&#125;</span> is running&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    processes = []</span><br><span class="line">    num_processes = <span class="number">4</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_processes):</span><br><span class="line">        p = multiprocessing.Process(target=worker, args=(i,))</span><br><span class="line">        processes.append(p)</span><br><span class="line">        p.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">        p.join()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;All workers are done&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<p>In this example, we first define a <code>worker</code> function,
which is the task that each subprocess will execute. Then in the main
process (if <strong>name</strong>==“<strong>main</strong>”), we create
<code>num_processes</code> subprocesses and add them to the
<code>processes</code> list. Next, we iterate through the
<code>processes</code> list, use the start() method from the Process
class to start each subprocess, and wait for them to complete. Finally,
we output “All workers are done” to indicate that all subprocesses have
finished execution.</p>
<h3 id="gpu">1.2 GPU</h3>
<p>Compared to CPUs, GPUs have more computing cores and higher
computational power, making them better suited for parallel computing.
Therefore, utilizing GPUs for code optimization can significantly
improve program execution efficiency. To use GPUs for code optimization
in Python, GPU programming frameworks such as NVIDIA’s CUDA framework
are commonly used.</p>
<h4 id="python-implementation-of-gpu-parallel-computing">1.2.1 Python
Implementation of GPU Parallel Computing</h4>
<p>CUDA, developed by NVIDIA, is a GPU programming framework that allows
developers to leverage the parallel computing capabilities of GPUs to
accelerate various compute-intensive tasks. CUDA provides a set of APIs
that facilitate GPU programming and supports multiple programming
languages, including C/C++, Python, and Java. When using CUDA, the CUDA
toolkit, which includes the CUDA driver, runtime library, and tools, is
required.</p>
<p>The following is a simple example of vector addition using the CUDA
framework: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numba <span class="keyword">import</span> cuda</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the vector addition function</span></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vector_add</span>(<span class="params">a, b, c</span>):</span><br><span class="line">    i = cuda.grid(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(c):</span><br><span class="line">        c[i] = a[i] + b[i]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the main program</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># Define the vector size</span></span><br><span class="line">    n = <span class="number">100000</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Generate random vectors on the host</span></span><br><span class="line">    a = np.random.randn(n).astype(np.float32)</span><br><span class="line">    b = np.random.randn(n).astype(np.float32)</span><br><span class="line">    c = np.zeros(n, dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Transfer data to GPU memory</span></span><br><span class="line">    d_a = cuda.to_device(a)</span><br><span class="line">    d_b = cuda.to_device(b)</span><br><span class="line">    d_c = cuda.to_device(c)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define thread blocks and the number of threads</span></span><br><span class="line">    threads_per_block = <span class="number">64</span></span><br><span class="line">    blocks_per_grid = (n + (threads_per_block - <span class="number">1</span>)) // threads_per_block</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Perform the vector addition operation</span></span><br><span class="line">    vector_add[blocks_per_grid, threads_per_block](d_a, d_b, d_c)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Transfer the result from GPU memory back to host memory</span></span><br><span class="line">    d_c.copy_to_host(c)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print the result</span></span><br><span class="line">    <span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure> In the above code, we first define a
<code>vector_add</code> function to add two vectors and store the result
in a third vector. Then we generate two random vectors and transfer them
to GPU memory. Next, we define thread blocks and the number of threads.
In the GPU section, we need to pay attention to vectorization, i.e.,
writing code that utilizes matrix operations rather than serial
computations with multiple for loops.</p>
<h4 id="using-gpu-to-train-neural-networks">1.2.2 Using GPU to Train
Neural Networks</h4>
<p>When it comes to training deep neural networks, GPUs can demonstrate
their advantages, as training neural networks often requires a large
number of matrix operations, which is precisely the task GPUs excel
at.</p>
<p>Here is a simple example of neural network training code based on
PyTorch and CUDA: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the neural network model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv1(x)))</span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc2(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the training data and labels</span></span><br><span class="line">inputs = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">labels = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move the neural network model to GPU</span></span><br><span class="line">net = Net().cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move the training data and labels to GPU</span></span><br><span class="line">inputs = inputs.cuda()</span><br><span class="line">labels = labels.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the loss function and optimizer</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outputs = net(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch %d, Loss: %.4f&#x27;</span> % (epoch+<span class="number">1</span>, loss.item()))</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>In the above code, we use the <code>xxx.cuda()</code> method to load
the model, training data, and labels into GPU memory, so that all
computations involved in the training loop are carried out on the
GPU.</p>
<h4 id="precautions">1.2.3 Precautions</h4>
<p>It’s particularly important to note that if you want to use a GPU for
acceleration, you must ensure that the code is vectorized. Simply put,
try to use matrix operations to represent the numerical computation
process instead of using multiple for loops in a nested manner.</p>
<p>This is because GPUs, compared to CPUs, are only stronger in terms of
parallelism, but their computational power is inferior to that of CPUs.
Serial computations like multiple for loops are not suitable for GPUs.
To put it in the words of Dr. Li Mu, if a CPU is like a college student,
a GPU is like a group of elementary school students. A college student
can handle tasks like calculus, while elementary school students can
only handle basic arithmetic. However, if a calculus task is broken down
into multiple basic arithmetic operations, then the advantage of a group
of elementary students, i.e., the GPU, becomes apparent.</p>
<p>To give a simple example, suppose we want to design a loss function
like this: <span class="math display">$$L_\theta=-\sum_{i=1}^{N}\sum_{j=1}^{N_i}w_{ij}log(p(x_i^j|M))$$</span>
If we don’t deliberately pay attention to the vectorization of the code,
a common-sense approach might look like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lossFunc</span>(<span class="params">y_pred,sols,objs</span>):</span><br><span class="line">batch size=y_pred.shape[<span class="number">0</span>]</span><br><span class="line">loss=torch.tensor(<span class="number">0.0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">	nSols=sols[i].shape[<span class="number">0</span>] <span class="comment">#Number of feasible solutions under the current batch (MIP)</span></span><br><span class="line">	nVars=sols[i].shape[<span class="number">1</span>]</span><br><span class="line">	<span class="comment">#Objective function normalization</span></span><br><span class="line">	den=objs[i].<span class="built_in">sum</span>()</span><br><span class="line">	<span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(objs[i].shape[<span class="number">0</span>]):</span><br><span class="line">		objs[i][l]=objs[i][l]/den</span><br><span class="line">	den=<span class="built_in">sum</span>(exp(-objs[i]))<span class="comment">#Calculate the denominator for wii coefficient</span></span><br><span class="line">	sum1=torch.tensor(<span class="number">0.0</span>)</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(nSols):</span><br><span class="line">		<span class="comment">#Calculate weight wij</span></span><br><span class="line">		w=exp(-objs[il[j])/den</span><br><span class="line">		<span class="comment">#Calculate the probability of feasible solution generation</span></span><br><span class="line">		P=torch.tensor(<span class="number">1.9</span>)</span><br><span class="line">		<span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(nVars):</span><br><span class="line">			<span class="keyword">if</span> sols[il[j,k]==<span class="number">1</span>:</span><br><span class="line">				P=p*y_pred[i][k]</span><br><span class="line">			<span class="keyword">elif</span> sols[il[j,k]==<span class="number">0</span>:</span><br><span class="line">				P=p*(<span class="number">1</span>-y_pred[i][k])</span><br><span class="line">		<span class="comment">#Calculate the summation</span></span><br><span class="line">		sum1+=w*p</span><br><span class="line">	loss+=sum1</span><br><span class="line">loss=-loss</span><br><span class="line"><span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>But in reality, a loss function designed with multiple nested for
loops is not suitable for GPU execution and may even perform worse than
on a CPU. Test results showed that with such a loss function, one epoch
would take about an hour…. making it impossible to train the neural
network.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lossFunc</span>(<span class="params">y,sols,objs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Loss function</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y : Neural network output batch_size x nVars</span></span><br><span class="line"><span class="string">    sols : Set of feasible solutions batch_size x nSols x nVars</span></span><br><span class="line"><span class="string">    objs : Objective function values corresponding to feasible solutions batch_size x nSols</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    loss : Loss on the current batch</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    objs=objs/<span class="number">15</span></span><br><span class="line">    eObjs=exp(-objs)</span><br><span class="line">    den=eObjs.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">    den=den.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    w=eObjs/den</span><br><span class="line">    y=y.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    p=y*sols+(<span class="number">1</span>-y)*(<span class="number">1</span>-sols)</span><br><span class="line">    p=log(p+<span class="number">1e-45</span>)</span><br><span class="line">    P=p.<span class="built_in">sum</span>(axis=<span class="number">2</span>)</span><br><span class="line">    loss=-(w*P).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>So the correct approach should be like this, which not only shows
significant acceleration on the GPU but also looks much cleaner… The
downside is that you have to constantly use the broadcasting mechanism
of matrix operations and it’s best to hand-calculate a few times with
small-scale examples, calculating while designing.</p>
<h2 id="compiler-acceleration">2 Compiler Acceleration</h2>
<h3 id="principles">2.1 Principles</h3>
<p>In computer programming, compiled languages and interpreted languages
are two common types. Compared to interpreted languages, compiled
languages execute faster because they need to compile the code into
executable binary code before execution. This is because compilers can
optimize the source code into more efficient machine code, thereby
speeding up program execution.</p>
<p>There are many ways for compiler optimization, among which the most
common include:</p>
<ol type="1">
<li><p>Eliminating unnecessary calculations: Compilers can identify
unnecessary calculations during code compilation, avoiding waste of
computational resources.</p></li>
<li><p>Loop unrolling: Loop unrolling refers to the practice of
repeating the code in the loop body several times to reduce the number
of loop iterations. This can improve the program’s running
speed.</p></li>
<li><p>Matrix/Vectorization: Matrix/Vectorization refers to placing
multiple data into a matrix or vector and then performing calculations
all at once. This can reduce the number of loop iterations and thus
improve the program’s running speed.</p></li>
</ol>
<p>To help developers conveniently utilize compiler optimization for
code, some open-source JIT compilers like Numba have been developed.
These compilers can convert Python and other interpreted language codes
into executable machine code, thus improving program execution
speed.</p>
<h3 id="python-acceleration-solution-based-on-numba">2.2 Python
Acceleration Solution Based on Numba</h3>
<p>Numba is an open-source JIT compiler that can convert Python code
into machine code, achieving code acceleration. Numba supports various
optimization techniques, including loop unrolling, code vectorization,
etc. Using Numba can greatly increase the execution speed of Python
code.</p>
<p>The following is a general code framework for using Numba to achieve
Python code acceleration: 1. Import the numba library 2. Define a
function that needs optimization 3. Use the <span class="citation" data-cites="numba.jit">@numba.jit</span> decorator to decorate the
function, generating the numba-optimized function 4. Call the optimized
function</p>
<p>To illustrate, let’s consider a comparative case that involves a
deeply nested for loop to compute the determinant of a matrix. This is a
compute-intensive operation that can benefit from acceleration using
Numba.</p>
<p>Original Python code: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">det</span>(<span class="params">matrix</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(matrix)</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> matrix[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">elif</span> n == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> matrix[<span class="number">0</span>][<span class="number">0</span>] * matrix[<span class="number">1</span>][<span class="number">1</span>] - matrix[<span class="number">0</span>][<span class="number">1</span>] * matrix[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            sub_matrix = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">                row = []</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                    <span class="keyword">if</span> k != j:</span><br><span class="line">                        row.append(matrix[i][k])</span><br><span class="line">                sub_matrix.append(row)</span><br><span class="line">            result += matrix[<span class="number">0</span>][j] * det(sub_matrix) * (-<span class="number">1</span>) ** j</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br></pre></td></tr></table></figure> As we can see, this function
contains deeply nested for loops, which are severely limited by the
performance of the Python interpreter. Now let’s use Numba to accelerate
it.</p>
<p>Optimized Numba code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numba</span><br><span class="line"></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">det</span>(<span class="params">matrix</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(matrix)</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> matrix[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">elif</span> n == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> matrix[<span class="number">0</span>][<span class="number">0</span>] * matrix[<span class="number">1</span>][<span class="number">1</span>] - matrix[<span class="number">0</span>][<span class="number">1</span>] * matrix[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            sub_matrix = np.zeros((n-<span class="number">1</span>, n-<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                    <span class="keyword">if</span> k != j:</span><br><span class="line">                        sub_matrix[i-<span class="number">1</span>, k-(k&gt;j)] = matrix[i, k]</span><br><span class="line">            result += matrix[<span class="number">0</span>][j] * det(sub_matrix) * (-<span class="number">1</span>) ** j</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Here, we use the <code>@numba.jit(nopython=True)</code> decorator to
declare the function as one that can be accelerated by Numba.
Simultaneously, we replace Python lists with Numpy arrays and use Numpy
array slicing and broadcasting features to reduce loops and memory
allocation.</p>
<p>Testing code: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate a random 10x10 matrix</span></span><br><span class="line">matrix = np.random.rand(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Time the original Python code</span></span><br><span class="line">start = time.time()</span><br><span class="line">d = det(matrix)</span><br><span class="line">end = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Python code took <span class="subst">&#123;end-start:<span class="number">.4</span>f&#125;</span> seconds, result=<span class="subst">&#123;d&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Time the Numba-optimized code</span></span><br><span class="line">start = time.time()</span><br><span class="line">d = det(matrix)</span><br><span class="line">end = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Numba-optimized code took <span class="subst">&#123;end-start:<span class="number">.4</span>f&#125;</span> seconds, result=<span class="subst">&#123;d&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<p>Testing results: <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Python <span class="selector-tag">code</span> took <span class="number">0.5960</span> seconds, result=-<span class="number">0.004127521725273144</span></span><br><span class="line">Numba-optimized <span class="selector-tag">code</span> took <span class="number">0.0040</span> seconds, result=-<span class="number">0.004127521725273144</span></span><br></pre></td></tr></table></figure></p>
<p>It’s important to note that Numba is not almighty, and there are
certain limitations to the types of code it can accelerate. If the
function decorated with the decorator is nested with many functions from
third-party libraries, Numba may not be able to work.</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#code-optimization"><span class="toc-number">2.</span> <span class="toc-text">Code Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#parallelization"><span class="toc-number">2.1.</span> <span class="toc-text">1 Parallelization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cpu-multi-process-operations"><span class="toc-number">2.1.1.</span> <span class="toc-text">1.1 CPU Multi-process
Operations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpu"><span class="toc-number">2.1.2.</span> <span class="toc-text">1.2 GPU</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#python-implementation-of-gpu-parallel-computing"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">1.2.1 Python
Implementation of GPU Parallel Computing</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#using-gpu-to-train-neural-networks"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">1.2.2 Using GPU to Train
Neural Networks</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#precautions"><span class="toc-number">2.1.2.3.</span> <span class="toc-text">1.2.3 Precautions</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#compiler-acceleration"><span class="toc-number">2.2.</span> <span class="toc-text">2 Compiler Acceleration</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#principles"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.1 Principles</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#python-acceleration-solution-based-on-numba"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2 Python
Acceleration Solution Based on Numba</span></a></li></ol></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&text=Accelerating Your Program Through Coding Skills"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&title=Accelerating Your Program Through Coding Skills"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&is_video=false&description=Accelerating Your Program Through Coding Skills"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Accelerating Your Program Through Coding Skills&body=Check out this article: https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&title=Accelerating Your Program Through Coding Skills"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&title=Accelerating Your Program Through Coding Skills"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&title=Accelerating Your Program Through Coding Skills"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&title=Accelerating Your Program Through Coding Skills"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&name=Accelerating Your Program Through Coding Skills&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/&t=Accelerating Your Program Through Coding Skills"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2025
    BigdogManLuo
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
