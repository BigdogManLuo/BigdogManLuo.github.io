{"posts":[{"title":"è£ç¼å¼å¼€å‘ï¼šç”¨MATLABæ‰¹é‡å‘é€ä¸€å°å›¾æ–‡å¹¶èŒ‚çš„é‚®ä»¶","text":"é—®é¢˜èƒŒæ™¯ å‰å‡ å¤©ï¼Œå¥³æœ‹å‹å…¬å¸è¦æ±‚å¥¹å‘¨æœ«åŠ ç­å‘é€å®£ä¼ æŽ¨å¹¿é‚®ä»¶ç»™é«˜æ ¡è€å¸ˆï¼Œè€Œæˆ‘æ‰çŸ¥æ™“ä¹‹å‰å¥¹ä»¬å‘é€é‚®ä»¶å…¨é æ‰‹åŠ¨â€¦ç›®å‰å¾…å‘åå•è‡³å°‘æœ‰ä¸‰åƒäººï¼Œè€ŒçŽ°åœ¨æ‰å‘äº†ä¸€ç™¾å¤šå°ï¼Œæˆ‘çš„å¿ƒæƒ…å¦‚ä¸‹ ä¸ºäº†èƒ½å¤Ÿä¸åŠ ç­å‘¨æœ«åŽ»æ”¾é£Žç­ï¼ŒäºŽæ˜¯æˆ‘å†³å®šäº²è‡ªåŠ¨æ‰‹ã€‚ä½†æ˜¯æ¯•ç«Ÿæˆ‘ä¸æ˜¯æžWebå¼€å‘çš„ï¼Œåº”è¯¥ä»Žå“ªä¸‹æ‰‹å‘¢ï¼Ÿç­”æ¡ˆæ˜¯ï¼šé¢å‘ç™¾åº¦ç¼–ç¨‹ã€‚ å®žçŽ°è¿‡ç¨‹ ç»„è£…è½®å­â€”â€”MATLABå¦‚ä½•å‘é€ä¸€å°å›¾æ–‡å¹¶èŒ‚çš„é‚®ä»¶ï¼Ÿ 1.å‘é€ä¸€å°æ™®é€šçš„é‚®ä»¶ MathWorks åœ¨å®˜æ–¹æ–‡æ¡£é‡Œæä¾›äº†sendmailå‡½æ•°ç”¨äºŽå‘æŒ‡å®šçš„åœ°å€åˆ—è¡¨å‘é€ç”µå­é‚®ä»¶ éœ€è¦è®¾ç½®çš„å‚æ•°å¦‚ä¸‹ï¼š å‘ä»¶äººé‚®ç®±åœ°å€ã€å‘ä»¶äººé‚®ç®±æŽˆæƒç ï¼ˆæ³¨æ„ä¸æ˜¯å¯†ç ï¼Œè€Œæ˜¯é‚®ç®±ç”¨äºŽç™»å½•ç¬¬ä¸‰æ–¹é‚®ä»¶å®¢æˆ·ç«¯çš„ä¸“ç”¨å¯†ç ï¼‰ã€SMTPæœåŠ¡å™¨åœ°å€ã€æ”¶ä»¶äººé‚®ç®±ã€ä¸»é¢˜ã€å†…å®¹ã€é™„ä»¶ è¯¦ç»†è¯·è§ï¼šhttps://ww2.mathworks.cn/help/matlab/ref/sendmail.html?searchHighlight=sendmail&amp;s_tid=srchtitle_sendmail_1ï¼ˆMathWorkså®˜æ–¹æ–‡æ¡£ï¼‰https://blog.csdn.net/eswai/article/details/53454987ï¼ˆä½¿ç”¨ä¾‹ç¨‹ï¼‰ 2.å‘é€htmlæ ¼å¼çš„é‚®ä»¶ mathworkså®˜æ–¹ç»™å‡ºçš„sendmailå‡½æ•°è™½ç„¶èƒ½å¤Ÿå®žçŽ°è‡ªåŠ¨å‘é€é‚®ä»¶ï¼Œä½†æœ‰ä¸ªè‡´å‘½çš„ç¼ºç‚¹æ˜¯ï¼Œå®ƒåªèƒ½æ”¯æŒæ™®é€šæ–‡æœ¬ï¼Œå¹¶ä¸èƒ½å‘é€htmlæ ¼å¼ï¼Œå› æ­¤æŽ’ç‰ˆã€å­—å·ã€é¢œè‰²ç­‰å‚æ•°ç»Ÿç»Ÿéƒ½ä¸èƒ½å®žçŽ°ã€‚è·ç¦»æˆ‘ä»¬çš„ç›®æ ‡è¿˜æœ‰ä¸€æ®µè·ç¦»ã€‚ ç»è¿‡äº’è”ç½‘çš„å¤§æµ·æžé’ˆï¼Œç»ˆäºŽæ‰¾åˆ°è¿™æ ·ä¸€ç¯‡å¸–å­ï¼Œæœ‰ä½è€å“¥æå‡ºæ¥å¯ä»¥ä¿®æ”¹sendmailçš„åº“å‡½æ•°å®žçŽ°å‘é€htmlé‚®ä»¶ã€‚ï¼ˆhttps://undocumentedmatlab.com/articles/sending-html-emails-from-matlabï¼‰ç¿»è¯‘è¿‡æ¥ä¸»è¦æœ‰ä¸¤ç‚¹ï¼š 1.HTML æ ¼å¼éœ€è¦è°ƒç”¨æ¶ˆæ¯å¯¹è±¡çš„ setContentï¼ˆï¼‰ æ–¹æ³•ï¼Œè€Œä¸æ˜¯ setTextï¼ˆï¼‰ 2. æˆ‘ä»¬éœ€è¦æŒ‡å®šä¸ºæ¶ˆæ¯ç¼–ç çš„ä¸€éƒ¨åˆ†â€™text/htmlâ€™ å½“ç„¶ï¼Œä¸ºäº†é¿å…é‡å¤é€ è½®å­ï¼Œæˆ‘ä»¬é€‰æ‹©ç›´æŽ¥ç™½å«–ï¼ˆç‹—å¤´ï¼‰ 3.å¦‚ä½•å†™htmlï¼Ÿ é‚£ä¹ˆé—®é¢˜åˆæ¥äº†ï¼Œè™½ç„¶æˆ‘ä»¬çŸ¥é“å¯ä»¥é€šè¿‡htmlæ ¼å¼å‘é€ä¸€å°æŽ’ç‰ˆæ•´é½ï¼Œå›¾æ–‡å¹¶èŒ‚çš„é‚®ä»¶ï¼Œå¯æ˜¯å¯¹äºŽå°ç™½æ¥è¯´ï¼Œè¦æƒ³é€šè¿‡htmlä¹¦å†™é‚®ä»¶å¹¶ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹æƒ…ã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥æ¢ä¸ªæ€è·¯ï¼Œå°†æ–‡æœ¬å†…å®¹è½¬æ¢ä¸ºhtmlã€‚ ç›´æŽ¥æ£€ç´¢æ–‡æœ¬è½¬htmlï¼Œä¼šå‘çŽ°æœ‰è®¸å¤šåœ¨çº¿å·¥å…·ï¼Œæ”¯æŒtxt,docxç­‰æ ¼å¼ã€‚ æ‰“å¼€è½¬æ¢å¥½çš„xxx.htmlæ–‡ä»¶åŽï¼ŒæŒ‰Ctrl+Uè¿›å…¥ç½‘é¡µåŽå°ï¼Œå°±å¯ä»¥çœ‹åˆ°æ–‡æœ¬å¯¹åº”çš„htmlä»£ç äº†ã€‚ 4.å‘é€å›¾ç‰‡ é‚£ä¹ˆé—®é¢˜åˆæ¥äº†ï¼Œå‡å¦‚æˆ‘ä»¬çš„é‚®ä»¶ä¸­å¸¦æœ‰å›¾ç‰‡å†…å®¹ï¼Œåˆä¸æƒ³é€šè¿‡é™„ä»¶çš„æ–¹å¼å‘é€ï¼Œè€Œæ˜¯åµŒå…¥åˆ°é‚®ä»¶ä¸­ï¼Œé€šè¿‡æ–‡æœ¬è½¬æ¢çš„æ–¹å¼èƒ½å¤Ÿç”Ÿæˆå›¾ç‰‡çš„htmlä»£ç å—ï¼Ÿç­”æ¡ˆæ˜¯å¯ä»¥çš„ï¼Œä½†æ˜¯ç”Ÿæˆçš„ä»£ç æ˜¯å›¾ç‰‡çš„é“¾æŽ¥ï¼Œåœ¨å‘é€é‚®ä»¶ä¹‹åŽï¼Œé‚®ä»¶ç³»ç»Ÿå¾€å¾€ä¼šå±è”½æŽ‰å›¾ç‰‡é“¾æŽ¥ï¼Œä»Žè€Œå›¾ç‰‡æ— æ³•æ˜¾ç¤ºã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œä¸€èˆ¬å¯ä»¥é€šè¿‡â€œCIDå†…åµŒå›¾åƒâ€æ–¹å¼å®Œæˆã€‚ CID çš„å·¥ä½œåŽŸç†æ˜¯å°†å›¾åƒé™„åŠ åˆ°è¦å‘é€çš„ç”µå­é‚®ä»¶ä¸­ï¼Œç„¶åŽä½¿ç”¨å¼•ç”¨è¯¥å›¾åƒçš„æ ‡å‡† HTMLå›¾åƒæ ‡è®°ï¼Œä»¥ä¾¿åœ¨ç”¨æˆ·æ‰“å¼€è¯¥å›¾åƒæ—¶æœ€ç»ˆå°†å…¶åµŒå…¥åˆ°ç”µå­é‚®ä»¶ä¸­ã€‚ ä½†æ˜¯ç”±äºŽè¯¥æ–¹æ³•åŒæ ·æ¶‰åŠåˆ°éœ€è¦æ”¹åŠ¨sendmailåº“å‡½æ•°åŠå…¶è°ƒç”¨çš„javaç±»è½¯ä»¶åŒ…ï¼Œå› æ­¤å¯¹å›¾åƒå‘½åcidæ ‡ç­¾æŒ‡å‘é™„ä»¶çš„æ–¹å¼è¾ƒä¸ºå¤æ‚ï¼ŒCSDNä¸­æœ‰å¤§éƒ¨åˆ†å¸–å­æ˜¯ç”¨pythonå®žçŽ°ï¼Œæœªæ›¾æ‰¾åˆ°matlabæ–¹æ³•ã€‚å°±åœ¨é¢˜ä¸»å¿«è¦å±±ç©·æ°´å°½ä¹‹æ—¶ï¼Œåˆæ‰¾åˆ°äº†è¿™æ ·ä¸€ç¯‡æ–‡ç« ï¼Œé‡Œé¢æå‡ºå¯ä»¥å°†å›¾ç‰‡ä»¥base64ç¼–ç çš„htmlä»£ç è¿›è¡Œé‚®ä»¶å‘é€ã€‚ ï¼ˆhttps://sendgrid.com/blog/embedding-images-emails-facts/ï¼‰ base64æ˜¯ä¸€ä¸ªå­—ç¬¦é›†ï¼Œå›¾ç‰‡åœ¨htmlä¸­å¯ä»¥é€šè¿‡base64ç¼–ç ä¸ºå­—ç¬¦ä¸²ï¼Œè€Œä¸ç”¨é€šè¿‡httpè¯·æ±‚ã€‚ä½†æ˜¯è¿™æ ·åšçš„ç¼ºç‚¹æ˜¯å›¾ç‰‡è¶Šå¤§ï¼Œbase64çš„å­—ç¬¦ä¸²å°±ä¼šè¶Šé•¿ï¼Œå¯¼è‡´å‘é€çš„htmlä»£ç è¶Šé•¿ï¼Œå¯¹ç½‘ç»œå¸¦å®½è¦æ±‚ä¹Ÿæ›´é«˜ã€‚ pythonå’Œjavaä¸­è‡ªå¸¦äº†å°†æ–‡ä»¶è½¬ç ä¸ºbase64çš„åº“å‡½æ•°ï¼Œä½†ç”±äºŽä¸€å¼€å§‹é€‰é”™äº†è·¯ï¼Œæ‰€ä»¥å°±åªèƒ½ç¡¬ç€å¤´çš®èµ°åˆ°é»‘äº†ã€‚ äºŽæ˜¯ï¼Œæˆ‘åˆæ‰¾åˆ°äº†å›¾ç‰‡è½¬base64ç¼–ç å·¥å…·ï¼Œæ‰‹åŠ¨è§£ç ï¼Œè‡ªæ­¤å¤§åŠŸå‘Šæˆã€‚ è§£æ”¾åŒæ‰‹â€”â€”å®žçŽ°æ‰¹é‡æ“ä½œ æˆ‘ä»¬æœ€ç»ˆçš„ç›®çš„æ˜¯ä¸ºäº†å®žçŽ°å†…å®¹ç›¸ä¼¼ï¼Œç§°å‘¼ä¸åŒçš„å®£ä¼ æŽ¨å¹¿é‚®ä»¶æ‰¹é‡å‘é€ã€‚çŽ°åœ¨ç»ˆäºŽå¯ä»¥å¼€å§‹æ–½å±•æ‹³è„šäº†ã€‚ åœ¨é…ç½®å¥½é‚®ç®±åŸºæœ¬ä¿¡æ¯ä¹‹åŽï¼Œè¯»å–ä¸€ä¸‹å¾…å‘é€äººå‘˜çš„åå•ï¼Œä»¥å˜é‡receiverNameå’ŒreceiverEmailå‚¨å­˜ã€‚ 1234567%æŽ¥æ”¶è€…ä¿¡æ¯è®¾ç½®receiver=readcell(&quot;xxxxx.xlsx&quot;); %æ–‡ä»¶receiver=string(receiver);for i=1:length(receiver)-1 receiverName(i,1)=receiver(i+1,1); receiverEmail(i,1)=receiver(i+1,2);end ç´§æŽ¥ç€è®¾ç½®ä¸»é¢˜å’Œå†…å®¹ï¼Œå…¶ä¸­å‡½æ•°strjoinå¯ä»¥å®žçŽ°ï¼Œå­—ç¬¦ä¸²æ‹¼æŽ¥åŠŸèƒ½ï¼Œç”¨æ¢è¡Œç¬¦åˆ†éš”htmlä»£ç ï¼Œé¿å…ä¸€è¡Œçš„ä»£ç è¿‡é•¿ï¼Œå†ç»Ÿä¸€æ ¹æ®æ®æ¢è¡Œç¬¦æ‹¼æŽ¥åœ¨ä¸€èµ·ã€‚ 123456789101112mailcontent=strjoin({ '&lt;html&gt;' '&lt;body&gt;' '&lt;h1&gt;Hello, World!&lt;/h1&gt;' '' '&lt;h2&gt;%sè€å¸ˆä½ å¥½ï¼&lt;/h2&gt;' '&lt;img src=&quot;å›¾ç‰‡çš„base64ç¼–ç &quot;&gt;' '' '&lt;/html&gt;' '&lt;/body&gt;' }, '\\n'); sprintfå‡½æ•°å¯ä»¥å®žçŽ°å­—ç¬¦æ›¿æ¢çš„åŠŸèƒ½ï¼Œåœ¨é‚®ä»¶å†…å®¹çš„htmlä»£ç ä¸­å°†ç§°å‘¼çš„åœ°æ–¹æ”¹ä¸º%s ä¾‹å¦‚å°Šæ•¬çš„%sè€å¸ˆï¼Œé€šè¿‡å‡½æ•°sprintfå¯ä»¥å°†%så¤„æ›¿æ¢ä¸ºå˜é‡receiverName(j) 123456789101112mailcontent=strjoin({ '&lt;html&gt;' '&lt;body&gt;' '&lt;h1&gt;Hello, World!&lt;/h1&gt;' '' '&lt;h2&gt;%sè€å¸ˆä½ å¥½ï¼&lt;/h2&gt;' '&lt;img src=&quot;å›¾ç‰‡çš„base64ç¼–ç &quot;&gt;' '' '&lt;/html&gt;' '&lt;/body&gt;' }, '\\n'); å‘é€é‚®ä»¶ï¼šç”±äºŽæ˜¯æ‰¹é‡å‘é€ï¼Œä¸”æ•°é‡ä¸€èˆ¬è¾ƒå¤§ï¼Œæœ€å¥½ä½¿ç”¨tryâ€¦catchâ€¦çš„è¯­æ³•ï¼Œå½“æŸä¸€æ¬¡å› ä¸ºé‚®ç®±ä¿¡æ¯é”™è¯¯ã€ç½‘ç»œå»¶è¿Ÿç­‰åŽŸå› sendmailå‡ºçŽ°æŠ¥é”™æ—¶èƒ½å¤Ÿè·³è¿‡æ­¤æ¬¡å‘é€å¹¶è®°å½•ä¸‹æ­¤æ¬¡é”™è¯¯ã€‚ 12345678910%å‘é€é‚®ä»¶ try sendmail_html(receiverEmail(j),mailtitle,mailcontent); catch error_count=error_count+1; error_index(j)=&quot;failed&quot;; %è®°å½•å¤±è´¥é‚®ä»¶çš„ç´¢å¼• end count=count-1;end å®Œæ•´ä»£ç ï¼š 123456789101112131415161718192021222324252627282930%æŽ¥æ”¶è€…ä¿¡æ¯è®¾ç½®error_count=0;error_index=&quot; &quot;;receiver=readcell(&quot;xxxxxx.xlsx&quot;);receiver=string(receiver);for i=1:length(receiver)-1 receiverName(i,1)=receiver(i+1,1); receiverEmail(i,1)=receiver(i+1,2);end%ä¸»é¢˜mailtitle='xxxxxxxxxxxxxxx';for j=1:length(receiverName)%æ­£æ–‡mailcontent=sprintf(strjoin({%æ­¤å¤„æ”¾å…¥htmlä»£ç  ä»¥æ¢è¡Œç¬¦åˆ†éš” é¿å…ä¸€è¡Œè¿‡é•¿ %æ”¶ä»¶äººæ˜µç§°ç”¨%sä»£æ›¿}ï¼Œ&quot;/n&quot;),receiverName(j))%å‘é€é‚®ä»¶ try sendmail_html(receiverEmail(j),mailtitle,mailcontent); catch error_count=error_count+1; error_index(j)=&quot;failed&quot;; %è®°å½•å¤±è´¥é‚®ä»¶çš„ç´¢å¼• end count=count-1;end é¢å‘å°ç™½â€”â€”UIäº¤äº’ æ—¢ç„¶éƒ½åšåˆ°è¿™äº†ï¼Œé‚£ä¹ˆå°±è¦é¢é›¶åŸºç¡€çš„ä»Žä¸šäººå‘˜è¿›è¡Œå¼€å‘ï¼Œè®¾è®¡äº¤äº’ç•Œé¢ï¼Œå®žçŽ°è¾“å…¥é‚®ä»¶åå•è‡ªåŠ¨å‘é€å¹¶ä¸”å¯è§†åŒ–çš„åŠŸèƒ½ï¼Œæ‰æ˜¯ä¸€æ¬¾çœŸæ­£çš„äº§å“ã€‚UIç•Œé¢é‡‡ç”¨MATLAB AppDesignerè¿›è¡Œè®¾è®¡ã€‚ å·¦ä¸Šè§’ä¸ºå‘ä»¶äººè®¾ç½®ç•Œé¢ï¼Œè¾“å…¥å‘ä»¶äººä¿¡æ¯è¿›è¡Œé‚®ç®±æŽˆæƒç éªŒè¯ï¼ŒæˆåŠŸåŽæ‰å¯ç™»å½•ã€‚å·¦ä¸‹è§’ä¸ºæ”¶ä»¶äººè®¾ç½®ï¼Œé€‰æ‹©æ”¶ä»¶äººåå•ï¼Œç‚¹å‡»ç¡®è®¤éªŒè¯æ ¼å¼æ— è¯¯åŽå³å¯åœ¨å‘é€çŠ¶æ€ç•Œé¢æ˜¾ç¤ºå‡ºå¾…å‘é€åå•ï¼ŒåŒæ—¶å¯ä»¥æ˜¾ç¤ºå‡ºå‘é€çš„çŠ¶æ€ã€‚ä¿—è¯è¯´ï¼Œç¨‹åºé‡Œ20%æ˜¯åœ¨å®Œæˆä»»åŠ¡ï¼Œ80%æ˜¯åœ¨é¢„é˜²å¯èƒ½å‡ºçŽ°çš„é—®é¢˜ã€‚è¿™é‡Œé¢çš„ç»†èŠ‚è¿˜è›®å¤šçš„ï¼Œä¾‹å¦‚å‘é€åŽåœ¨åå•æœªå‘é€å®Œä¹‹å‰ä¸å¯é‡å¤å‘é€ï¼Œå‘é€å®ŒæˆåŽæ•´ç†å‘é€å¤±è´¥åå•ï¼Œå¯å†æ¬¡ç‚¹å‡»â€œå‘é€â€ç­‰ã€‚è¿™äº›é—®é¢˜çš„å‘çŽ°ä¸Žå¤„ç†ï¼Œå°±åªèƒ½é ç¨‹åºå‘˜æ˜Žé”çš„ç›´è§‰äº†â€¦ç›®å‰è¿™æ¬¾äº§å“è¿˜æœªå•†ç”¨ï¼Œä»…ä¾›è‡ªå¨±è‡ªä¹ã€‚ å¦é™„ä¸€å¼ æ”¾é£Žç­çš„ç…§ç‰‡ï¼Œè§£æ”¾åŒæ‰‹ï¼","link":"/2022/04/15/techArt-AutoEmail/"},{"title":"Accelerating Your Program Through Coding Skills","text":"Introduction When dealing with large-scale, complex optimization problems or training neural networks, we often encounter situations where programs run for extended periods or fail to complete. However, this is not necessarily due to the large problem scale or limitations of computer hardware capabilities. Even when attempting to use higher-performance servers or computers, thereâ€™s no guarantee of effectively accelerating code execution. This is because high-performance hardware typically needs to be matched with code designed for high-performance computing. This article aims to provide some code-level optimization strategies for program acceleration. By optimizing code structure and designing high-performance computing solutions, we can effectively accelerate program execution and improve runtime efficiency. Itâ€™s important to note that this article only covers code-level acceleration solutions and does not include optimization measures related to algorithms, hardware, etc. The article is written based on personal experience, and if there are any shortcomings, please point them out. Code Optimization Simply put, there are mainly two approaches to implementing program optimization. One is to parallelize tasks and write parallel code to leverage the parallel computing capabilities of multi-core CPUs or GPUs to accelerate program execution. The other is to utilize compiler code optimization mechanisms to compile parts of code that require interpreters (such as Python, MATLAB) into machine code, achieving faster program execution speeds. 1 Parallelization Parallelization requires coordination between software and hardware, but the prerequisite is that the overall task can be decomposed into subtasks that can be executed simultaneously. There are two ways to achieve parallelization: one relies on multi-core CPUs to implement multi-process operations, and the other relies on GPUs. We will introduce the implementation methods of these two approaches below. 1.1 CPU Multi-process Operations Multi-process operations leverage the multi-core characteristics of CPUs to achieve parallel computing. In multi-process operations, programs are decomposed into multiple subtasks, each running in an independent process. These processes can execute different tasks in parallel, thereby accelerating program execution. Multi-process operations can be implemented using Pythonâ€™s multiprocessing library. When using the multiprocessing library for multi-process operations, itâ€™s first necessary to decompose tasks into multiple subtasks and assign each subtask to different processes for execution. The following is a code framework for the multiprocessing library: 123456789101112131415161718import multiprocessingdef worker(num): print(f&quot;Worker {num} is running&quot;) returnif __name__ == &quot;__main__&quot;: processes = [] num_processes = 4 for i in range(num_processes): p = multiprocessing.Process(target=worker, args=(i,)) processes.append(p) p.start() for p in processes: p.join() print(&quot;All workers are done&quot;) In this example, we first define a worker function, which is the task that each subprocess will execute. Then in the main process (if name==â€œmainâ€), we create num_processes subprocesses and add them to the processes list. Next, we iterate through the processes list, use the start() method from the Process class to start each subprocess, and wait for them to complete. Finally, we output â€œAll workers are doneâ€ to indicate that all subprocesses have finished execution. 1.2 GPU Compared to CPUs, GPUs have more computing cores and higher computational power, making them better suited for parallel computing. Therefore, utilizing GPUs for code optimization can significantly improve program execution efficiency. To use GPUs for code optimization in Python, GPU programming frameworks such as NVIDIAâ€™s CUDA framework are commonly used. 1.2.1 Python Implementation of GPU Parallel Computing CUDA, developed by NVIDIA, is a GPU programming framework that allows developers to leverage the parallel computing capabilities of GPUs to accelerate various compute-intensive tasks. CUDA provides a set of APIs that facilitate GPU programming and supports multiple programming languages, including C/C++, Python, and Java. When using CUDA, the CUDA toolkit, which includes the CUDA driver, runtime library, and tools, is required. The following is a simple example of vector addition using the CUDA framework: 12345678910111213141516171819202122232425262728293031323334353637import numpy as npfrom numba import cuda# Define the vector addition function@cuda.jitdef vector_add(a, b, c): i = cuda.grid(1) if i &lt; len(c): c[i] = a[i] + b[i]# Define the main programif __name__ == '__main__': # Define the vector size n = 100000 # Generate random vectors on the host a = np.random.randn(n).astype(np.float32) b = np.random.randn(n).astype(np.float32) c = np.zeros(n, dtype=np.float32) # Transfer data to GPU memory d_a = cuda.to_device(a) d_b = cuda.to_device(b) d_c = cuda.to_device(c) # Define thread blocks and the number of threads threads_per_block = 64 blocks_per_grid = (n + (threads_per_block - 1)) // threads_per_block # Perform the vector addition operation vector_add[blocks_per_grid, threads_per_block](d_a, d_b, d_c) # Transfer the result from GPU memory back to host memory d_c.copy_to_host(c) # Print the result print(c) In the above code, we first define a vector_add function to add two vectors and store the result in a third vector. Then we generate two random vectors and transfer them to GPU memory. Next, we define thread blocks and the number of threads. In the GPU section, we need to pay attention to vectorization, i.e., writing code that utilizes matrix operations rather than serial computations with multiple for loops. 1.2.2 Using GPU to Train Neural Networks When it comes to training deep neural networks, GPUs can demonstrate their advantages, as training neural networks often requires a large number of matrix operations, which is precisely the task GPUs excel at. Here is a simple example of neural network training code based on PyTorch and CUDA: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import torchimport torch.nn as nnimport torch.optim as optim# Define the neural network modelclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x# Define the training data and labelsinputs = torch.randn(1, 3, 32, 32)labels = torch.randn(1, 10)# Move the neural network model to GPUnet = Net().cuda()# Move the training data and labels to GPUinputs = inputs.cuda()labels = labels.cuda()# Define the loss function and optimizercriterion = nn.MSELoss()optimizer = optim.SGD(net.parameters(), lr=0.01)# Start trainingfor epoch in range(100): optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() print('Epoch %d, Loss: %.4f' % (epoch+1, loss.item())) In the above code, we use the xxx.cuda() method to load the model, training data, and labels into GPU memory, so that all computations involved in the training loop are carried out on the GPU. 1.2.3 Precautions Itâ€™s particularly important to note that if you want to use a GPU for acceleration, you must ensure that the code is vectorized. Simply put, try to use matrix operations to represent the numerical computation process instead of using multiple for loops in a nested manner. This is because GPUs, compared to CPUs, are only stronger in terms of parallelism, but their computational power is inferior to that of CPUs. Serial computations like multiple for loops are not suitable for GPUs. To put it in the words of Dr. Li Mu, if a CPU is like a college student, a GPU is like a group of elementary school students. A college student can handle tasks like calculus, while elementary school students can only handle basic arithmetic. However, if a calculus task is broken down into multiple basic arithmetic operations, then the advantage of a group of elementary students, i.e., the GPU, becomes apparent. To give a simple example, suppose we want to design a loss function like this: $$L_\\theta=-\\sum_{i=1}^{N}\\sum_{j=1}^{N_i}w_{ij}log(p(x_i^j|M))$$ If we donâ€™t deliberately pay attention to the vectorization of the code, a common-sense approach might look like this: 123456789101112131415161718192021222324252627def lossFunc(y_pred,sols,objs):batch size=y_pred.shape[0]loss=torch.tensor(0.0)for i in range(batch_size): nSols=sols[i].shape[0] #Number of feasible solutions under the current batch (MIP) nVars=sols[i].shape[1] #Objective function normalization den=objs[i].sum() for l in range(objs[i].shape[0]): objs[i][l]=objs[i][l]/den den=sum(exp(-objs[i]))#Calculate the denominator for wii coefficient sum1=torch.tensor(0.0) for j in range(nSols): #Calculate weight wij w=exp(-objs[il[j])/den #Calculate the probability of feasible solution generation P=torch.tensor(1.9) for k in range(nVars): if sols[il[j,k]==1: P=p*y_pred[i][k] elif sols[il[j,k]==0: P=p*(1-y_pred[i][k]) #Calculate the summation sum1+=w*p loss+=sum1loss=-lossreturn loss But in reality, a loss function designed with multiple nested for loops is not suitable for GPU execution and may even perform worse than on a CPU. Test results showed that with such a loss function, one epoch would take about an hourâ€¦ making it impossible to train the neural network. 123456789101112131415161718192021222324def lossFunc(y,sols,objs): &quot;&quot;&quot; Loss function Parameters ---------- y : Neural network output batch_size x nVars sols : Set of feasible solutions batch_size x nSols x nVars objs : Objective function values corresponding to feasible solutions batch_size x nSols Returns ------- loss : Loss on the current batch &quot;&quot;&quot; objs=objs/15 eObjs=exp(-objs) den=eObjs.sum(axis=1) den=den.unsqueeze(1) w=eObjs/den y=y.unsqueeze(1) p=y*sols+(1-y)*(1-sols) p=log(p+1e-45) P=p.sum(axis=2) loss=-(w*P).sum() return loss So the correct approach should be like this, which not only shows significant acceleration on the GPU but also looks much cleanerâ€¦ The downside is that you have to constantly use the broadcasting mechanism of matrix operations and itâ€™s best to hand-calculate a few times with small-scale examples, calculating while designing. 2 Compiler Acceleration 2.1 Principles In computer programming, compiled languages and interpreted languages are two common types. Compared to interpreted languages, compiled languages execute faster because they need to compile the code into executable binary code before execution. This is because compilers can optimize the source code into more efficient machine code, thereby speeding up program execution. There are many ways for compiler optimization, among which the most common include: Eliminating unnecessary calculations: Compilers can identify unnecessary calculations during code compilation, avoiding waste of computational resources. Loop unrolling: Loop unrolling refers to the practice of repeating the code in the loop body several times to reduce the number of loop iterations. This can improve the programâ€™s running speed. Matrix/Vectorization: Matrix/Vectorization refers to placing multiple data into a matrix or vector and then performing calculations all at once. This can reduce the number of loop iterations and thus improve the programâ€™s running speed. To help developers conveniently utilize compiler optimization for code, some open-source JIT compilers like Numba have been developed. These compilers can convert Python and other interpreted language codes into executable machine code, thus improving program execution speed. 2.2 Python Acceleration Solution Based on Numba Numba is an open-source JIT compiler that can convert Python code into machine code, achieving code acceleration. Numba supports various optimization techniques, including loop unrolling, code vectorization, etc. Using Numba can greatly increase the execution speed of Python code. The following is a general code framework for using Numba to achieve Python code acceleration: Import the numba library Define a function that needs optimization Use the @numba.jit decorator to decorate the function, generating the numba-optimized function Call the optimized function To illustrate, letâ€™s consider a comparative case that involves a deeply nested for loop to compute the determinant of a matrix. This is a compute-intensive operation that can benefit from acceleration using Numba. Original Python code: 12345678910111213141516171819def det(matrix): n = len(matrix) if n == 1: return matrix[0][0] elif n == 2: return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0] else: result = 0 for j in range(n): sub_matrix = [] for i in range(1, n): row = [] for k in range(n): if k != j: row.append(matrix[i][k]) sub_matrix.append(row) result += matrix[0][j] * det(sub_matrix) * (-1) ** j return result As we can see, this function contains deeply nested for loops, which are severely limited by the performance of the Python interpreter. Now letâ€™s use Numba to accelerate it. Optimized Numba code: 1234567891011121314151617181920import numba@numba.jit(nopython=True)def det(matrix): n = len(matrix) if n == 1: return matrix[0][0] elif n == 2: return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0] else: result = 0 for j in range(n): sub_matrix = np.zeros((n-1, n-1)) for i in range(1, n): for k in range(n): if k != j: sub_matrix[i-1, k-(k&gt;j)] = matrix[i, k] result += matrix[0][j] * det(sub_matrix) * (-1) ** j return result Here, we use the @numba.jit(nopython=True) decorator to declare the function as one that can be accelerated by Numba. Simultaneously, we replace Python lists with Numpy arrays and use Numpy array slicing and broadcasting features to reduce loops and memory allocation. Testing code: 1234567891011121314151617import timeimport numpy as np# Generate a random 10x10 matrixmatrix = np.random.rand(10, 10)# Time the original Python codestart = time.time()d = det(matrix)end = time.time()print(f&quot;Python code took {end-start:.4f} seconds, result={d}&quot;)# Time the Numba-optimized codestart = time.time()d = det(matrix)end = time.time()print(f&quot;Numba-optimized code took {end-start:.4f} seconds, result={d}&quot;) Testing results: 12Python code took 0.5960 seconds, result=-0.004127521725273144Numba-optimized code took 0.0040 seconds, result=-0.004127521725273144 Itâ€™s important to note that Numba is not almighty, and there are certain limitations to the types of code it can accelerate. If the function decorated with the decorator is nested with many functions from third-party libraries, Numba may not be able to work.","link":"/2023/03/06/techArt-Code_acc/"},{"title":"Try to Understand the Nature of Forecasting (1)â€”â€”Form Geocentric Model to Heliocentric Model","text":"One evening at a jazz bar in Liverpool, I have an impromptu â€œbattleâ€ with Kaggle Grand Master Z. Lin about â€œTraining a Neural Networkâ€. We picked one of the regression datasets, ran three rounds, and compared average results. In the end, I won with a lower test loss. Looking back, the key difference came down to activation functions. I noticed that the data distribution showed clear discontinuities, so using ReLU (with its natural truncation) will fit the data-generating process better. It reduced overfitting and allowed me to get away with fewer layers. Afterward, our conversation turned philosophical: what is the essence of â€œpredictionâ€? We used the transition from the geocentric model to the heliocentric model as an example. Both models tried to explain the real physical system. The geocentric model was like endlessly updating rules with new observational data, like online incremental learning on an over-parameterized model. Even with more observed data, the risk of overfitting never went away. The heliocentric model, by contrast, had far fewer parameters. It not only fit past data but also predicted future observations remarkably well. Later, the model was extended with Newtonâ€™s law of gravitation â€” a perfect case of how a simpler model can generalize better. This raised the bigger question: why does simplicity so often prevail? Why do we invoke Occamâ€™s razor, or use L1/L2 regularization to constrain model complexity? Why is it that most physical systems we want to predict arenâ€™t wildly over-parameterized in the first place? That question is much harder to answer. In the end, we fully agreed a good prediction method should approximate the natural data-generating process as closely as possible. That way, it saves parameter amount and reduces the risk of overfitting. But we also came to another conclusion: the essence of â€œpredictionâ€ is not really about faithfully reconstructing physical reality. Instead, it is an engineering trick aimed at generalization, floating somewhere above the actual system. Why exactly this is the case is something Iâ€™ll leave for a future blog post, where Iâ€™ll try to explain the idea of an â€œimpossible triangleâ€ of forecasting.","link":"/2025/08/25/thoughts-the_nature_of_forecast/"},{"title":"è®º AIGC çš„&quot;å›¾è…¾&quot;","text":"å½“å‰ AIGC å·²ç»å¯ä»¥åšåˆ°ä»¥å‡ä¹±çœŸçš„ç¨‹åº¦ï¼Œä¾‹å¦‚ç”¨ AI P å›¾æ¥éª—å–ç½‘è´­é€€æ¬¾ç­‰ç­‰æ¡ˆä¾‹ã€‚å¯é¢„è§çš„æœªæ¥æœ‰ä¸€å¤©ï¼Œä¸ç®¡æ˜¯ç…§ç‰‡ã€éŸ³é¢‘è¿˜æ˜¯è§†é¢‘ï¼Œéƒ½æ— æ³•ä½œä¸ºæ³•åº­ä¸Šå®¢è§‚çš„è¯æ®å‘ˆçŽ°äº†ã€‚ å¼ºåˆ¶çš„æš—æ°´å° å…³é”®åœ¨äºŽå¦‚ä½•è¾¨åˆ« AI äº§ç”Ÿæˆ–è€…å¤„ç†è¿‡çš„å†…å®¹ã€‚å¯¹äºŽå¯ä»¥å…¬å¼€è¢«è®¿é—®çš„ AI æ¨¡åž‹ï¼Œå¯ä»¥å¼ºåˆ¶å…¶ç”Ÿæˆå†…å®¹ä¸­å¸¦æœ‰æš—æ°´å°ï¼Œä¸ä¼šå½±å“è§†è§‰ï¼Œä½†å®žçŽ°äº†ä¿ç•™ä¸€ä¸ªæŽ¥å£ä¾›å¤–éƒ¨å¯ç›´æŽ¥é‰´å®šã€‚å½“ç„¶ï¼Œä¹Ÿä¼šç›¸åº”çš„ä»¿æš—æ°´å°æŠ€æœ¯å°†çœŸå®žå†…å®¹ä¼ªé€ ä¸º AI å†…å®¹ã€‚è™½ä¸èƒ½ä»¥å‡ä¹±çœŸï¼Œä½†ä¹Ÿæä¾›äº†ä¸€ä¸ªé€”å¾„å°†çœŸå˜ä¸ºâ€œå‡â€ã€‚ä¸è¿‡ç”¨éžå¯¹ç§°åŠ å¯†åº”è¯¥å¯ä»¥è§£å†³è¿™ä¸€ç‚¹ã€‚ä½†å³ä¾¿è¿™ä¸ªå¼ºåˆ¶æŽªæ–½å¯ä»¥æ²»ç†å¸‚é¢ä¸Šå¯å…¬å¼€è®¿é—®æ¨¡åž‹ï¼Œä½†ä»ç„¶æ— æ³•é¿å…ç§æœ‰çš„ AI æ¨¡åž‹åšä¼ªï¼Œæ¯•ç«ŸæŠ€æœ¯æ°¸è¿œéƒ½æ˜¯ä¸€æŠŠåŒåˆƒå‰‘ã€‚ å›¾è…¾ é™¤äº†æš—æ°´å°è¿™ç§æ²»æ ‡çš„æ–¹æ³•ï¼Œæ˜¯å¦è¿˜æœ‰æŠ€æœ¯å¤Ÿç›´æŽ¥æ£€æµ‹ AIGCï¼Ÿä¸€æ¬¡åœ¨æ¢¦ä¸­çš„çµæ„Ÿï¼Œè”æƒ³åˆ°ç›—æ¢¦ç©ºé—´ä¸­çš„â€œå›¾è…¾â€ï¼Œé™€èžºåœä¸‹æ¥å°±ä»£è¡¨èº«å¤„çŽ°å®žä¸–ç•Œï¼Œè€Œé™€èžºä¸€ç›´ä¸åœè½¬åˆ™ä»£è¡¨å¤„äºŽæ¢¦ä¸­ã€‚è¿™æ˜¯å› ä¸ºçŽ°å®žä¸–ç•Œé‡Œæ ¹æœ¬ä¸å­˜åœ¨ä¸€ç›´ä¸åœè½¬çš„é™€èžºã€‚ æ‰€ä»¥å…³é”®åœ¨äºŽç‰©ç†è§„å¾‹ã€‚å¦‚æžœä¸€ä¸ªè§†é¢‘æœ‰æ•ˆï¼Œé‚£ä¹ˆå®ƒåº”è¯¥åŒ…å«æœ‰å¯è¢«æ£€æµ‹çš„â€å›¾è…¾â€œã€‚å¯¹äºŽè§†é¢‘é¢†åŸŸï¼Œç¬¦åˆç‰©ç†è§„å¾‹çš„è¿åŠ¨å¾€å¾€è¦é€šè¿‡ç‰©ç†å¼•æ“Žè¿›è¡Œæ¸²æŸ“ã€‚å°¤å…¶æ˜¯å¯¹äºŽæµä½“çš„æ¨¡æ‹Ÿï¼Œåˆ™éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´å’Œè®¡ç®—æˆæœ¬åŽ»åšæ•°å€¼ä»¿çœŸï¼ˆä¸”ç”±äºŽæ··æ²Œç‰¹æ€§ï¼Œæµä½“çš„æ•°å€¼è®¡ç®—ä¹Ÿå¯èƒ½ä¸Žå®žé™…çš„æƒ…å†µä¹Ÿå­˜åœ¨å¤§å‡ºå…¥ï¼‰ã€‚è€ŒçœŸå®žä¸–ç•Œçš„è¿åŠ¨æ˜¯é€šè¿‡çœŸå®žçš„ç‰©ç†â€å¼•æ“Žâ€œæ¸²æŸ“å‡ºæ¥çš„ï¼Œè‡ªç„¶å‘ç”Ÿçš„ã€‚å³ä¾¿å›žåˆ°ç‰©ç†å¼•æ“Žçš„æ¨¡æ‹Ÿï¼Œä¹Ÿä¸ŽçŽ°æœ‰çš„ diffusion model è¿™ç±»çš„ GAI èŒƒå¼æœ‰ç€æœ¬è´¨çš„åŒºåˆ«ã€‚ä¸€ä¸ªæ˜¯ model-based è®¡ç®—ï¼Œä¸€ä¸ªæ˜¯ data-driven å¼çš„æ‰©æ•£ã€‚ç›®å‰å¾ˆéš¾åœ¨ç”Ÿæˆå¼çš„ç¥žç»ç½‘ç»œæ¨¡åž‹ä¸­ç›´æŽ¥æ·»åŠ ä¸€ä¸ªç¡¬ç‰©ç†çº¦æŸï¼Œæ›´ä½•å†µæ˜¯å¤æ‚çš„æ— æ•°æ¡ç‰©ç†å®šå¾‹ã€‚æ‰€ä»¥ï¼Œè¦æƒ³è¯æ˜Žä¸€ä¸ªçœŸå®žä¸–ç•Œçš„è§†é¢‘ï¼Œè€Œä¸æ˜¯ AI äº§ç”Ÿçš„ï¼Œæˆ–è®¸åªéœ€è¦åœ¨è§†é¢‘é‡Œå‡ºçŽ°é‚£äº›ç¬¦åˆç‰©ç†è§„å¾‹çš„è¿åŠ¨ã€‚æˆ‘æƒ³è¿™å°±æ˜¯ AIGC çš„â€œå›¾è…¾â€å§ã€‚å½“ç„¶ï¼Œè¿™ä»…é™äºŽè§†é¢‘ï¼Œè€Œç±»ä¼¼äºŽå›¾ç‰‡ï¼ŒéŸ³é¢‘è¿™æ ·çš„å†…å®¹å¯èƒ½è¿˜éœ€è¦ç»§ç»­å¯»æ‰¾â€œå›¾è…¾â€ã€‚ä½†æˆ‘è®¤ä¸ºï¼Œä¸ç®¡å¦‚ä½•ï¼Œè¿™ä¸ªâ€œå›¾è…¾â€åº”è¯¥ç¬¦åˆçš„æ ¸å¿ƒåŽŸç†æ˜¯ï¼š åœ¨çœŸå®žä¸–ç•Œä¸­å¾ˆå®¹æ˜“äº§ç”Ÿçš„ åœ¨è®¡ç®—ä¸–ç•Œé‡Œéœ€è¦è€—è´¹å¤§é‡æˆæœ¬å’Œæ—¶é—´æ‰èƒ½æ¨¡æ‹Ÿå‡ºæ¥ï¼ˆä½†å§‹ç»ˆä¸æ˜¯çœŸå®žä¸–ç•Œå®Œå…¨ä¸€è‡´çš„ï¼‰ ä½†æ˜¯å¯ä»¥é€šè¿‡æœ‰é™çš„æˆæœ¬å¯ä»¥è¢«æ£€æµ‹å‡ºæ¥çš„ (ç±»ä¼¼äºŽhashå‡½æ•°è¿™æ ·çš„å•å‘é€šé“) å½“ç„¶ï¼Œç†æƒ³çš„å›¾è…¾è¿˜æ˜¯éš¾ä»¥å¯»æ‰¾ã€‚ä¸ç®¡æ˜¯æš—æ°´å°è¿˜æ˜¯â€œå›¾è…¾â€ï¼Œå…¶æ ¸å¿ƒç›®çš„éƒ½æ˜¯ä¸ºäº†å¢žåŠ é€ å‡çš„æˆæœ¬ã€‚åªä¸è¿‡æš—æ°´å°æ— æ³•æœç»æ ¹æœ¬ï¼Œè€Œâ€œå›¾è…¾â€åˆ™æ˜¯åˆ©ç”¨äº† GAI èŒƒå¼ä¸ŽçŽ°å®žä¸–ç•Œä¸­æœ¬è´¨çš„åŒºåˆ«ã€‚AI åº”ç”¨æ²»ç†è§„èŒƒå·²ç»è¿«åœ¨çœ‰ç«ï¼Œä½†ä»ç„¶æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚","link":"/2025/08/06/thoughts-AIGC/"}],"tags":[],"categories":[{"name":"Articles","slug":"Articles","link":"/categories/Articles/"},{"name":"Thoughts","slug":"Thoughts","link":"/categories/Thoughts/"}],"pages":[{"title":"Chuanqing Pu","text":"PhD Candidate, Electrical Engineering Shanghai Jiao Tong University ðŸ“§ sashabanks@sjtu.edu.cn Â· ðŸ™ GitHub Â· ðŸŽ“ Google Scholar Â· ðŸ“ CV ðŸ”¬ Research Focus Learning-Augmented Optimization Â· Decision-Focused Learning Â· Probabilistic Forecasting Â· Energy Trading Â· Battery Storage Â· Port Powerâ€“Logistics Systems ðŸŽ“ Education Ph.D. in Electrical Engineering, Shanghai Jiao Tong University, 2023 â€“ Present Research Interest: Learning-based Optimization for Energy Systems B.Eng. in Electrical Engineering, Sichuan University, 2019 â€“ 2023 ðŸ”¬ Research / Engineering Experience Online Aggregated Windâ€“Solar Probabilistic Forecasting and Trading in GB Electricity Market ðŸ“… 2023.12 â€“ 2024.12 We proposed a comprehensive solution for the IEEE Hybrid Energy Forecasting and Trading Competition. Our methods achieve accurate probabilistic forecasts for a windâ€“solar hybrid system and substantial trading revenue in the day-ahead electricity market. Key contributions: Stacking-based ensemble of multi-source NWP forecasts for wind power Online post-processing for solar capacity shifts Probabilistic aggregation for hybrid quantile forecasts Stochastic trading strategy under uncertain prices Keywords: wind forecasting Â· solar forecasting Â· probabilistic aggregation Â· trading strategy ðŸ”— Practical Code â€“ engineering-oriented version that can be deployed and run online ðŸ”— Research Code â€“ reproducible version aligned with the paper, providing one-click execution of case studies ðŸ”— Paper â€¹ Workflow overview Forecasting & trading results â€º Learning-based Optimization in Seaport Power-Logistics System The operation of modern seaports features a strong coupling between energy supply and logistics demand. Optimizing such energy-logistics scheduling problems is typically formulated as large-scale mixed-integer programming, which is computationally expensive and must be solved repeatedly on a daily basis. Our work proposes a decision surrogate model for integer variable prediction in centralized scheduling and warm-start model for distributed scheduling in seaport power-logistics coupled scheduling. Both approaches improve computational efficiency while ensuring feasibility of the scheduling results, and the warm-start approach additionally guarantees the optimality. ðŸ”— Paper â€¹ Workflow overview â€º ðŸ† Awards &amp; Honors IEEE Hybrid Energy Forecasting and Trading Competition (HEFTCom24) â€“ Best Student Team Award (1st) ðŸ“… 2023.12 â€“ 2024.07 Ranked 3rd (trading) and 4th (forecasting) overall, and 1st student team Invited to present at ISF 2024 ðŸ”— Practical Code â€“ engineering-oriented version that can be deployed and run online ðŸ”— Research Code â€“ reproducible version aligned with the paper, providing one-click execution of case studies ðŸ”— Paper 2025 THS Forecasting Hackathon â€“ Grand Prize (1st Place) ðŸ“… 2025.07 7-hour on-site programming challenge on forecasting quarterly tourist arrivals for HK/Macau Achieved lowest MAE among all teams ðŸ”— Code (coming soon) â€œTI Cupâ€ National Undergraduate Electronic Design Competition â€“ National 1st Prize ðŸ“… 2020.09 â€“ 2021.12 Developed DSP-based control algorithms for three-phase inverter &amp; rectifier Implemented PLL, PID, PR controllers, and dq/Î±Î² transforms ðŸ”— DSP Tools Â· PCB Source Â· Demo Video PandaFPE 2023 â€“ Best Paper Award ðŸ“… 2023.05 Proposed a two-stage coordination framework for wind farms with ESS Integrated day-ahead DP and real-time rolling optimization for energy &amp; frequency regulation markets ðŸ”— Code ðŸ›  Skills ðŸ§® Optimization Modeling ðŸ”¥ PyTorch ðŸ“ CVXPY ðŸ“Š Time Series âš¡ Energy Systems ðŸ“ˆ ADMM ðŸ§  MIP / MINLP","link":"/about/index.html"},{"title":"","text":"(function () { function setupSlider(slider) { const track = slider.querySelector('.slider-track'); const slides = slider.querySelectorAll('.slide'); const prevBtn = slider.querySelector('.slider-btn.prev'); const nextBtn = slider.querySelector('.slider-btn.next'); if (!track || slides.length === 0) return; let index = 0; function update() { track.style.transform = `translateX(-${index * 100}%)`; if (prevBtn) prevBtn.disabled = (index === 0); if (nextBtn) nextBtn.disabled = (index === slides.length - 1); } function go(step) { index = Math.max(0, Math.min(slides.length - 1, index + step)); update(); } prevBtn && prevBtn.addEventListener('click', () => go(-1)); nextBtn && nextBtn.addEventListener('click', () => go(1)); // é”®ç›˜æ”¯æŒ slider.setAttribute('tabindex', '0'); slider.addEventListener('keydown', (e) => { if (e.key === 'ArrowLeft') { e.preventDefault(); go(-1); } if (e.key === 'ArrowRight') { e.preventDefault(); go(1); } }); // è§¦æ‘¸æ»‘åŠ¨ let startX = 0, dx = 0, touching = false; const threshold = 40; slider.addEventListener('touchstart', (e) => { touching = true; startX = e.touches[0].clientX; dx = 0; }, {passive:true}); slider.addEventListener('touchmove', (e) => { if (!touching) return; dx = e.touches[0].clientX - startX; }, {passive:true}); slider.addEventListener('touchend', () => { if (!touching) return; if (dx > threshold) go(-1); else if (dx < -threshold) go(1); touching = false; dx = 0; }); update(); } function initAll() { document.querySelectorAll('.slider').forEach(setupSlider); } if (document.readyState === 'loading') { document.addEventListener('DOMContentLoaded', initAll); } else { initAll(); } document.addEventListener('pjax:complete', initAll); })();","link":"/about/js/custom.js"}]}