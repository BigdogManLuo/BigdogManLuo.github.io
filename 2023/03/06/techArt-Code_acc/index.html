<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Accelerating Your Program Through Coding Skills - Try to Understand</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Try to Understand"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Try to Understand"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Introduction When dealing with large-scale, complex optimization problems or training neural networks, we often encounter situations where programs run for extended periods or fail to complete. Howeve"><meta property="og:type" content="blog"><meta property="og:title" content="Accelerating Your Program Through Coding Skills"><meta property="og:url" content="https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/"><meta property="og:site_name" content="Try to Understand"><meta property="og:description" content="Introduction When dealing with large-scale, complex optimization problems or training neural networks, we often encounter situations where programs run for extended periods or fail to complete. Howeve"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://bigdogmanluo.github.io/img/og_image.png"><meta property="article:published_time" content="2023-03-05T16:00:00.000Z"><meta property="article:modified_time" content="2025-07-19T15:21:49.760Z"><meta property="article:author" content="BigdogManLuo"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://bigdogmanluo.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/"},"headline":"Accelerating Your Program Through Coding Skills","image":["https://bigdogmanluo.github.io/img/og_image.png"],"datePublished":"2023-03-05T16:00:00.000Z","dateModified":"2025-07-19T15:21:49.760Z","author":{"@type":"Person","name":"BigdogManLuo"},"publisher":{"@type":"Organization","name":"Try to Understand","logo":{"@type":"ImageObject","url":"https://bigdogmanluo.github.io/img/logo.svg"}},"description":"Introduction When dealing with large-scale, complex optimization problems or training neural networks, we often encounter situations where programs run for extended periods or fail to complete. Howeve"}</script><link rel="canonical" href="https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Try to Understand" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/BigdogManLuo"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-05T16:00:00.000Z" title="2023/3/6 00:00:00">2023-03-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-07-19T15:21:49.760Z" title="2025/7/19 23:21:49">2025-07-19</time></span><span class="level-item"><a class="link-muted" href="/categories/Technical-Articles/">Technical Articles</a></span><span class="level-item">15 minutes read (About 2324 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Accelerating Your Program Through Coding Skills</h1><div class="content"><h1>Introduction</h1>
<p>When dealing with large-scale, complex optimization problems or training neural networks, we often encounter situations where programs run for extended periods or fail to complete. However, this is not necessarily due to the large problem scale or limitations of computer hardware capabilities. Even when attempting to use higher-performance servers or computers, there’s no guarantee of effectively accelerating code execution. This is because high-performance hardware typically needs to be matched with code designed for high-performance computing.</p>
<p>This article aims to provide some code-level optimization strategies for program acceleration. By optimizing code structure and designing high-performance computing solutions, we can effectively accelerate program execution and improve runtime efficiency. It’s important to note that this article only covers code-level acceleration solutions and does not include optimization measures related to algorithms, hardware, etc. The article is written based on personal experience, and if there are any shortcomings, please point them out.</p>
<h1>Code Optimization</h1>
<p>Simply put, there are mainly two approaches to implementing program optimization. One is to parallelize tasks and write parallel code to leverage the parallel computing capabilities of multi-core CPUs or GPUs to accelerate program execution. The other is to utilize compiler code optimization mechanisms to compile parts of code that require interpreters (such as Python, MATLAB) into machine code, achieving faster program execution speeds.</p>
<h2 id="1-Parallelization">1 Parallelization</h2>
<p>Parallelization requires coordination between software and hardware, but the prerequisite is that the overall task can be decomposed into subtasks that can be executed simultaneously. There are two ways to achieve parallelization: one relies on multi-core CPUs to implement multi-process operations, and the other relies on GPUs. We will introduce the implementation methods of these two approaches below.</p>
<h3 id="1-1-CPU-Multi-process-Operations">1.1 CPU Multi-process Operations</h3>
<p>Multi-process operations leverage the multi-core characteristics of CPUs to achieve parallel computing. In multi-process operations, programs are decomposed into multiple subtasks, each running in an independent process. These processes can execute different tasks in parallel, thereby accelerating program execution. Multi-process operations can be implemented using Python’s multiprocessing library.</p>
<p>When using the multiprocessing library for multi-process operations, it’s first necessary to decompose tasks into multiple subtasks and assign each subtask to different processes for execution.</p>
<p>The following is a code framework for the multiprocessing library:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">worker</span>(<span class="params">num</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Worker <span class="subst">&#123;num&#125;</span> is running&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    processes = []</span><br><span class="line">    num_processes = <span class="number">4</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_processes):</span><br><span class="line">        p = multiprocessing.Process(target=worker, args=(i,))</span><br><span class="line">        processes.append(p)</span><br><span class="line">        p.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">        p.join()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;All workers are done&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>In this example, we first define a <code>worker</code> function, which is the task that each subprocess will execute. Then in the main process (if <strong>name</strong>==“<strong>main</strong>”), we create <code>num_processes</code> subprocesses and add them to the <code>processes</code> list. Next, we iterate through the <code>processes</code> list, use the start() method from the Process class to start each subprocess, and wait for them to complete. Finally, we output “All workers are done” to indicate that all subprocesses have finished execution.</p>
<h3 id="1-2-GPU">1.2 GPU</h3>
<p>Compared to CPUs, GPUs have more computing cores and higher computational power, making them better suited for parallel computing. Therefore, utilizing GPUs for code optimization can significantly improve program execution efficiency. To use GPUs for code optimization in Python, GPU programming frameworks such as NVIDIA’s CUDA framework are commonly used.</p>
<h4 id="1-2-1-Python-Implementation-of-GPU-Parallel-Computing">1.2.1 Python Implementation of GPU Parallel Computing</h4>
<p>CUDA, developed by NVIDIA, is a GPU programming framework that allows developers to leverage the parallel computing capabilities of GPUs to accelerate various compute-intensive tasks. CUDA provides a set of APIs that facilitate GPU programming and supports multiple programming languages, including C/C++, Python, and Java. When using CUDA, the CUDA toolkit, which includes the CUDA driver, runtime library, and tools, is required.</p>
<p>The following is a simple example of vector addition using the CUDA framework:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numba <span class="keyword">import</span> cuda</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the vector addition function</span></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vector_add</span>(<span class="params">a, b, c</span>):</span><br><span class="line">    i = cuda.grid(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(c):</span><br><span class="line">        c[i] = a[i] + b[i]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the main program</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># Define the vector size</span></span><br><span class="line">    n = <span class="number">100000</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Generate random vectors on the host</span></span><br><span class="line">    a = np.random.randn(n).astype(np.float32)</span><br><span class="line">    b = np.random.randn(n).astype(np.float32)</span><br><span class="line">    c = np.zeros(n, dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Transfer data to GPU memory</span></span><br><span class="line">    d_a = cuda.to_device(a)</span><br><span class="line">    d_b = cuda.to_device(b)</span><br><span class="line">    d_c = cuda.to_device(c)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define thread blocks and the number of threads</span></span><br><span class="line">    threads_per_block = <span class="number">64</span></span><br><span class="line">    blocks_per_grid = (n + (threads_per_block - <span class="number">1</span>)) // threads_per_block</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Perform the vector addition operation</span></span><br><span class="line">    vector_add[blocks_per_grid, threads_per_block](d_a, d_b, d_c)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Transfer the result from GPU memory back to host memory</span></span><br><span class="line">    d_c.copy_to_host(c)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print the result</span></span><br><span class="line">    <span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure>
<p>In the above code, we first define a <code>vector_add</code> function to add two vectors and store the result in a third vector. Then we generate two random vectors and transfer them to GPU memory. Next, we define thread blocks and the number of threads. In the GPU section, we need to pay attention to vectorization, i.e., writing code that utilizes matrix operations rather than serial computations with multiple for loops.</p>
<h4 id="1-2-2-Using-GPU-to-Train-Neural-Networks">1.2.2 Using GPU to Train Neural Networks</h4>
<p>When it comes to training deep neural networks, GPUs can demonstrate their advantages, as training neural networks often requires a large number of matrix operations, which is precisely the task GPUs excel at.</p>
<p>Here is a simple example of neural network training code based on PyTorch and CUDA:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the neural network model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv1(x)))</span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc2(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the training data and labels</span></span><br><span class="line">inputs = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">labels = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move the neural network model to GPU</span></span><br><span class="line">net = Net().cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move the training data and labels to GPU</span></span><br><span class="line">inputs = inputs.cuda()</span><br><span class="line">labels = labels.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the loss function and optimizer</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    outputs = net(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch %d, Loss: %.4f&#x27;</span> % (epoch+<span class="number">1</span>, loss.item()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>In the above code, we use the <code>xxx.cuda()</code> method to load the model, training data, and labels into GPU memory, so that all computations involved in the training loop are carried out on the GPU.</p>
<h4 id="1-2-3-Precautions">1.2.3 Precautions</h4>
<p>It’s particularly important to note that if you want to use a GPU for acceleration, you must ensure that the code is vectorized. Simply put, try to use matrix operations to represent the numerical computation process instead of using multiple for loops in a nested manner.</p>
<p>This is because GPUs, compared to CPUs, are only stronger in terms of parallelism, but their computational power is inferior to that of CPUs. Serial computations like multiple for loops are not suitable for GPUs. To put it in the words of Dr. Li Mu, if a CPU is like a college student, a GPU is like a group of elementary school students. A college student can handle tasks like calculus, while elementary school students can only handle basic arithmetic. However, if a calculus task is broken down into multiple basic arithmetic operations, then the advantage of a group of elementary students, i.e., the GPU, becomes apparent.</p>
<p>To give a simple example, suppose we want to design a loss function like this:
$$L_\theta=-\sum_{i=1}^{N}\sum_{j=1}^{N_i}w_{ij}log(p(x_i^j|M))$$
If we don’t deliberately pay attention to the vectorization of the code, a common-sense approach might look like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lossFunc</span>(<span class="params">y_pred,sols,objs</span>):</span><br><span class="line">batch size=y_pred.shape[<span class="number">0</span>]</span><br><span class="line">loss=torch.tensor(<span class="number">0.0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">	nSols=sols[i].shape[<span class="number">0</span>] <span class="comment">#Number of feasible solutions under the current batch (MIP)</span></span><br><span class="line">	nVars=sols[i].shape[<span class="number">1</span>]</span><br><span class="line">	<span class="comment">#Objective function normalization</span></span><br><span class="line">	den=objs[i].<span class="built_in">sum</span>()</span><br><span class="line">	<span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(objs[i].shape[<span class="number">0</span>]):</span><br><span class="line">		objs[i][l]=objs[i][l]/den</span><br><span class="line">	den=<span class="built_in">sum</span>(exp(-objs[i]))<span class="comment">#Calculate the denominator for wii coefficient</span></span><br><span class="line">	sum1=torch.tensor(<span class="number">0.0</span>)</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(nSols):</span><br><span class="line">		<span class="comment">#Calculate weight wij</span></span><br><span class="line">		w=exp(-objs[il[j])/den</span><br><span class="line">		<span class="comment">#Calculate the probability of feasible solution generation</span></span><br><span class="line">		P=torch.tensor(<span class="number">1.9</span>)</span><br><span class="line">		<span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(nVars):</span><br><span class="line">			<span class="keyword">if</span> sols[il[j,k]==<span class="number">1</span>:</span><br><span class="line">				P=p*y_pred[i][k]</span><br><span class="line">			<span class="keyword">elif</span> sols[il[j,k]==<span class="number">0</span>:</span><br><span class="line">				P=p*(<span class="number">1</span>-y_pred[i][k])</span><br><span class="line">		<span class="comment">#Calculate the summation</span></span><br><span class="line">		sum1+=w*p</span><br><span class="line">	loss+=sum1</span><br><span class="line">loss=-loss</span><br><span class="line"><span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>But in reality, a loss function designed with multiple nested for loops is not suitable for GPU execution and may even perform worse than on a CPU. Test results showed that with such a loss function, one epoch would take about an hour… making it impossible to train the neural network.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lossFunc</span>(<span class="params">y,sols,objs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Loss function</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    y : Neural network output batch_size x nVars</span></span><br><span class="line"><span class="string">    sols : Set of feasible solutions batch_size x nSols x nVars</span></span><br><span class="line"><span class="string">    objs : Objective function values corresponding to feasible solutions batch_size x nSols</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    loss : Loss on the current batch</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    objs=objs/<span class="number">15</span></span><br><span class="line">    eObjs=exp(-objs)</span><br><span class="line">    den=eObjs.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">    den=den.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    w=eObjs/den</span><br><span class="line">    y=y.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    p=y*sols+(<span class="number">1</span>-y)*(<span class="number">1</span>-sols)</span><br><span class="line">    p=log(p+<span class="number">1e-45</span>)</span><br><span class="line">    P=p.<span class="built_in">sum</span>(axis=<span class="number">2</span>)</span><br><span class="line">    loss=-(w*P).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>So the correct approach should be like this, which not only shows significant acceleration on the GPU but also looks much cleaner… The downside is that you have to constantly use the broadcasting mechanism of matrix operations and it’s best to hand-calculate a few times with small-scale examples, calculating while designing.</p>
<h2 id="2-Compiler-Acceleration">2 Compiler Acceleration</h2>
<h3 id="2-1-Principles">2.1 Principles</h3>
<p>In computer programming, compiled languages and interpreted languages are two common types. Compared to interpreted languages, compiled languages execute faster because they need to compile the code into executable binary code before execution. This is because compilers can optimize the source code into more efficient machine code, thereby speeding up program execution.</p>
<p>There are many ways for compiler optimization, among which the most common include:</p>
<ol>
<li>
<p>Eliminating unnecessary calculations: Compilers can identify unnecessary calculations during code compilation, avoiding waste of computational resources.</p>
</li>
<li>
<p>Loop unrolling: Loop unrolling refers to the practice of repeating the code in the loop body several times to reduce the number of loop iterations. This can improve the program’s running speed.</p>
</li>
<li>
<p>Matrix/Vectorization: Matrix/Vectorization refers to placing multiple data into a matrix or vector and then performing calculations all at once. This can reduce the number of loop iterations and thus improve the program’s running speed.</p>
</li>
</ol>
<p>To help developers conveniently utilize compiler optimization for code, some open-source JIT compilers like Numba have been developed. These compilers can convert Python and other interpreted language codes into executable machine code, thus improving program execution speed.</p>
<h3 id="2-2-Python-Acceleration-Solution-Based-on-Numba">2.2 Python Acceleration Solution Based on Numba</h3>
<p>Numba is an open-source JIT compiler that can convert Python code into machine code, achieving code acceleration. Numba supports various optimization techniques, including loop unrolling, code vectorization, etc. Using Numba can greatly increase the execution speed of Python code.</p>
<p>The following is a general code framework for using Numba to achieve Python code acceleration:</p>
<ol>
<li>Import the numba library</li>
<li>Define a function that needs optimization</li>
<li>Use the @numba.jit decorator to decorate the function, generating the numba-optimized function</li>
<li>Call the optimized function</li>
</ol>
<p>To illustrate, let’s consider a comparative case that involves a deeply nested for loop to compute the determinant of a matrix. This is a compute-intensive operation that can benefit from acceleration using Numba.</p>
<p>Original Python code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">det</span>(<span class="params">matrix</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(matrix)</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> matrix[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">elif</span> n == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> matrix[<span class="number">0</span>][<span class="number">0</span>] * matrix[<span class="number">1</span>][<span class="number">1</span>] - matrix[<span class="number">0</span>][<span class="number">1</span>] * matrix[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            sub_matrix = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">                row = []</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                    <span class="keyword">if</span> k != j:</span><br><span class="line">                        row.append(matrix[i][k])</span><br><span class="line">                sub_matrix.append(row)</span><br><span class="line">            result += matrix[<span class="number">0</span>][j] * det(sub_matrix) * (-<span class="number">1</span>) ** j</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>As we can see, this function contains deeply nested for loops, which are severely limited by the performance of the Python interpreter. Now let’s use Numba to accelerate it.</p>
<p>Optimized Numba code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numba</span><br><span class="line"></span><br><span class="line"><span class="meta">@numba.jit(<span class="params">nopython=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">det</span>(<span class="params">matrix</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(matrix)</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> matrix[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">elif</span> n == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> matrix[<span class="number">0</span>][<span class="number">0</span>] * matrix[<span class="number">1</span>][<span class="number">1</span>] - matrix[<span class="number">0</span>][<span class="number">1</span>] * matrix[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            sub_matrix = np.zeros((n-<span class="number">1</span>, n-<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                    <span class="keyword">if</span> k != j:</span><br><span class="line">                        sub_matrix[i-<span class="number">1</span>, k-(k&gt;j)] = matrix[i, k]</span><br><span class="line">            result += matrix[<span class="number">0</span>][j] * det(sub_matrix) * (-<span class="number">1</span>) ** j</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Here, we use the <code>@numba.jit(nopython=True)</code> decorator to declare the function as one that can be accelerated by Numba. Simultaneously, we replace Python lists with Numpy arrays and use Numpy array slicing and broadcasting features to reduce loops and memory allocation.</p>
<p>Testing code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate a random 10x10 matrix</span></span><br><span class="line">matrix = np.random.rand(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Time the original Python code</span></span><br><span class="line">start = time.time()</span><br><span class="line">d = det(matrix)</span><br><span class="line">end = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Python code took <span class="subst">&#123;end-start:<span class="number">.4</span>f&#125;</span> seconds, result=<span class="subst">&#123;d&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Time the Numba-optimized code</span></span><br><span class="line">start = time.time()</span><br><span class="line">d = det(matrix)</span><br><span class="line">end = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Numba-optimized code took <span class="subst">&#123;end-start:<span class="number">.4</span>f&#125;</span> seconds, result=<span class="subst">&#123;d&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>Testing results:</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Python <span class="selector-tag">code</span> took <span class="number">0.5960</span> seconds, result=-<span class="number">0.004127521725273144</span></span><br><span class="line">Numba-optimized <span class="selector-tag">code</span> took <span class="number">0.0040</span> seconds, result=-<span class="number">0.004127521725273144</span></span><br></pre></td></tr></table></figure>
<p>It’s important to note that Numba is not almighty, and there are certain limitations to the types of code it can accelerate. If the function decorated with the decorator is nested with many functions from third-party libraries, Numba may not be able to work.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Accelerating Your Program Through Coding Skills</p><p><a href="https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/">https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>BigdogManLuo</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-03-06</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-07-19</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2025/08/06/thoughts-AIGC/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">论 AIGC 的&quot;图腾&quot;</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/04/15/techArt-AutoEmail/"><span class="level-item">裁缝式开发：用MATLAB批量发送一封图文并茂的邮件</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/head.png" alt="Chuanqing Pu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chuanqing Pu</p><p class="is-size-6 is-block">An explorer in Engineering</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">0</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/BigdogManLuo" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/BigdogManLuo"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="LinkedIn" href="https://www.linkedin.com/in/chuanqing-pu-943673317/"><i class="fab fa-linkedin"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Technical-Articles/"><span class="level-start"><span class="level-item">Technical Articles</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Thoughts/"><span class="level-start"><span class="level-item">Thoughts</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-08-05T16:00:00.000Z">2025-08-06</time></p><p class="title"><a href="/2025/08/06/thoughts-AIGC/">论 AIGC 的&quot;图腾&quot;</a></p><p class="categories"><a href="/categories/Thoughts/">Thoughts</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-05T16:00:00.000Z">2023-03-06</time></p><p class="title"><a href="/2023/03/06/techArt-Code_acc/">Accelerating Your Program Through Coding Skills</a></p><p class="categories"><a href="/categories/Technical-Articles/">Technical Articles</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-14T16:00:00.000Z">2022-04-15</time></p><p class="title"><a href="/2022/04/15/techArt-AutoEmail/">裁缝式开发：用MATLAB批量发送一封图文并茂的邮件</a></p><p class="categories"><a href="/categories/Technical-Articles/">Technical Articles</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/08/"><span class="level-start"><span class="level-item">August 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Try to Understand" height="28"></a><p class="is-size-7"><span>&copy; 2025 BigdogManLuo</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/BigdogManLuo"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>