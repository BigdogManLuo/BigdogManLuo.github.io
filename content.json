{"posts":[{"title":"裁缝式开发：用MATLAB批量发送一封图文并茂的邮件","text":"问题背景前几天，女朋友公司要求她周末加班发送宣传推广邮件给高校老师，而我才知晓之前她们发送邮件全靠手动…目前待发名单至少有三千人，而现在才发了一百多封，我的心情如下 为了能够不加班周末去放风筝，于是我决定亲自动手。但是毕竟我不是搞Web开发的，应该从哪下手呢？答案是：面向百度编程。 实现过程组装轮子——MATLAB如何发送一封图文并茂的邮件？1.发送一封普通的邮件MathWorks 在官方文档里提供了sendmail函数用于向指定的地址列表发送电子邮件 需要设置的参数如下：发件人邮箱地址、发件人邮箱授权码（注意不是密码，而是邮箱用于登录第三方邮件客户端的专用密码）、SMTP服务器地址、收件人邮箱、主题、内容、附件 详细请见：https://ww2.mathworks.cn/help/matlab/ref/sendmail.html?searchHighlight=sendmail&amp;s_tid=srchtitle_sendmail_1（MathWorks官方文档）https://blog.csdn.net/eswai/article/details/53454987（使用例程） 2.发送html格式的邮件mathworks官方给出的sendmail函数虽然能够实现自动发送邮件，但有个致命的缺点是，它只能支持普通文本，并不能发送html格式，因此排版、字号、颜色等参数统统都不能实现。距离我们的目标还有一段距离。经过互联网的大海捞针，终于找到这样一篇帖子，有位老哥提出来可以修改sendmail的库函数实现发送html邮件。（https://undocumentedmatlab.com/articles/sending-html-emails-from-matlab）翻译过来主要有两点：1.HTML 格式需要调用消息对象的 setContent（） 方法，而不是 setText（） 2. 我们需要指定为消息编码的一部分’text/html’ 当然，为了避免重复造轮子，我们选择直接白嫖（狗头） 3.如何写html？那么问题又来了，虽然我们知道可以通过html格式发送一封排版整齐，图文并茂的邮件，可是对于小白来说，要想通过html书写邮件并不是一件容易的事情。但是我们可以换个思路，将文本内容转换为html。直接检索文本转html，会发现有许多在线工具，支持txt,docx等格式。打开转换好的xxx.html文件后，按Ctrl+U进入网页后台，就可以看到文本对应的html代码了。 4.发送图片那么问题又来了，假如我们的邮件中带有图片内容，又不想通过附件的方式发送，而是嵌入到邮件中，通过文本转换的方式能够生成图片的html代码吗？答案是可以的，但是生成的代码是图片的链接，在发送邮件之后，邮件系统往往会屏蔽掉图片链接，从而图片无法显示。针对此问题，一般可以通过“CID内嵌图像”方式完成。 CID 的工作原理是将图像附加到要发送的电子邮件中，然后使用引用该图像的标准 HTML图像标记，以便在用户打开该图像时最终将其嵌入到电子邮件中。 但是由于该方法同样涉及到需要改动sendmail库函数及其调用的java类软件包，因此对图像命名cid标签指向附件的方式较为复杂，CSDN中有大部分帖子是用python实现，未曾找到matlab方法。就在题主快要山穷水尽之时，又找到了这样一篇文章，里面提出可以将图片以base64编码的html代码进行邮件发送。（https://sendgrid.com/blog/embedding-images-emails-facts/）base64是一个字符集，图片在html中可以通过base64编码为字符串，而不用通过http请求。但是这样做的缺点是图片越大，base64的字符串就会越长，导致发送的html代码越长，对网络带宽要求也更高。python和java中自带了将文件转码为base64的库函数，但由于一开始选错了路，所以就只能硬着头皮走到黑了。于是，我又找到了图片转base64编码工具，手动解码，自此大功告成。 解放双手——实现批量操作我们最终的目的是为了实现内容相似，称呼不同的宣传推广邮件批量发送。现在终于可以开始施展拳脚了。在配置好邮箱基本信息之后，读取一下待发送人员的名单，以变量receiverName和receiverEmail储存。 1234567%接收者信息设置receiver=readcell(&quot;xxxxx.xlsx&quot;); %文件receiver=string(receiver);for i=1:length(receiver)-1 receiverName(i,1)=receiver(i+1,1); receiverEmail(i,1)=receiver(i+1,2);end 紧接着设置主题和内容，其中函数strjoin可以实现，字符串拼接功能，用换行符分隔html代码，避免一行的代码过长，再统一根据据换行符拼接在一起。 123456789101112mailcontent=strjoin({ '&lt;html&gt;' '&lt;body&gt;' '&lt;h1&gt;Hello, World!&lt;/h1&gt;' '' '&lt;h2&gt;%s老师你好！&lt;/h2&gt;' '&lt;img src=&quot;图片的base64编码&quot;&gt;' '' '&lt;/html&gt;' '&lt;/body&gt;' }, '\\n'); sprintf函数可以实现字符替换的功能，在邮件内容的html代码中将称呼的地方改为%s 例如尊敬的%s老师，通过函数sprintf可以将%s处替换为变量receiverName(j) 123456789101112mailcontent=strjoin({ '&lt;html&gt;' '&lt;body&gt;' '&lt;h1&gt;Hello, World!&lt;/h1&gt;' '' '&lt;h2&gt;%s老师你好！&lt;/h2&gt;' '&lt;img src=&quot;图片的base64编码&quot;&gt;' '' '&lt;/html&gt;' '&lt;/body&gt;' }, '\\n'); 发送邮件：由于是批量发送，且数量一般较大，最好使用try…catch..的语法，当某一次因为邮箱信息错误、网络延迟等原因sendmail出现报错时能够跳过此次发送并记录下此次错误。 12345678910%发送邮件 try sendmail_html(receiverEmail(j),mailtitle,mailcontent); catch error_count=error_count+1; error_index(j)=&quot;failed&quot;; %记录失败邮件的索引 end count=count-1;end 完整代码： 123456789101112131415161718192021222324252627282930%接收者信息设置error_count=0;error_index=&quot; &quot;;receiver=readcell(&quot;xxxxxx.xlsx&quot;);receiver=string(receiver);for i=1:length(receiver)-1 receiverName(i,1)=receiver(i+1,1); receiverEmail(i,1)=receiver(i+1,2);end%主题mailtitle='xxxxxxxxxxxxxxx';for j=1:length(receiverName)%正文mailcontent=sprintf(strjoin({%此处放入html代码 以换行符分隔 避免一行过长 %收件人昵称用%s代替}，&quot;/n&quot;),receiverName(j))%发送邮件 try sendmail_html(receiverEmail(j),mailtitle,mailcontent); catch error_count=error_count+1; error_index(j)=&quot;failed&quot;; %记录失败邮件的索引 end count=count-1;end 面向小白——UI交互既然都做到这了，那么就要面零基础的从业人员进行开发，设计交互界面，实现输入邮件名单自动发送并且可视化的功能，才是一款真正的产品。UI界面采用MATLAB AppDesigner进行设计。 左上角为发件人设置界面，输入发件人信息进行邮箱授权码验证，成功后才可登录。左下角为收件人设置，选择收件人名单，点击确认验证格式无误后即可在发送状态界面显示出待发送名单，同时可以显示出发送的状态。俗话说，程序里20%是在完成任务，80%是在预防可能出现的问题。这里面的细节还蛮多的，例如发送后在名单未发送完之前不可重复发送，发送完成后整理发送失败名单，可再次点击“发送”等。这些问题的发现与处理，就只能靠程序员明锐的直觉了…目前这款产品还未商用，仅供自娱自乐。 另附一张放风筝的照片，解放双手！","link":"/2022/04/15/techArt-AutoEmail/"},{"title":"论 AIGC 的&quot;图腾&quot;","text":"当前 AIGC 已经可以做到以假乱真的程度，例如用 AI P 图来骗取网购退款等等案例。可预见的未来有一天，不管是照片、音频还是视频，都无法作为法庭上客观的证据呈现了。 强制的暗水印关键在于如何辨别 AI 产生或者处理过的内容。对于可以公开被访问的 AI 模型，可以强制其生成内容中带有暗水印，不会影响视觉，但实现了保留一个接口供外部可直接鉴定。当然，也会相应的仿暗水印技术将真实内容伪造为 AI 内容。虽不能以假乱真，但也提供了一个途径将真变为“假”。不过用非对称加密应该可以解决这一点。但即便这个强制措施可以治理市面上可公开访问模型，但仍然无法避免私有的 AI 模型做伪，毕竟技术永远都是一把双刃剑。 图腾除了暗水印这种治标的方法，是否还有技术够直接检测 AIGC？一次在梦中的灵感，联想到盗梦空间中的“图腾”，陀螺停下来就代表身处现实世界，而陀螺一直不停转则代表处于梦中。这是因为现实世界里根本不存在一直不停转的陀螺。 所以关键在于物理规律。如果一个视频有效，那么它应该包含有可被检测的”图腾“。对于视频领域，符合物理规律的运动往往要通过物理引擎进行渲染。尤其是对于流体的模拟，则需要花费大量时间和计算成本去做数值仿真（且由于混沌特性，流体的数值计算也可能与实际的情况也存在大出入）。而真实世界的运动是通过真实的物理”引擎“渲染出来的，自然发生的。即便回到物理引擎的模拟，也与现有的 diffusion model 这类的 GAI 范式有着本质的区别。一个是 model-based 计算，一个是 data-driven 式的扩散。目前很难在生成式的神经网络模型中直接添加一个硬物理约束，更何况是复杂的无数条物理定律。所以，要想证明一个真实世界的视频，而不是 AI 产生的，或许只需要在视频里出现那些符合物理规律的运动。我想这就是 AIGC 的“图腾”吧。当然，这仅限于视频，而类似于图片，音频这样的内容可能还需要继续寻找“图腾”。但我认为，不管如何，这个“图腾”应该符合的核心原理是： 在真实世界中很容易产生的 在计算世界里需要耗费大量成本和时间才能模拟出来（但始终不是真实世界完全一致的） 但是可以通过有限的成本可以被检测出来的 (类似于hash函数这样的单向通道) 当然，理想的图腾还是难以寻找。不管是暗水印还是“图腾”，其核心目的都是为了增加造假的成本。只不过暗水印无法杜绝根本，而“图腾”则是利用了 GAI 范式与现实世界中本质的区别。AI 应用治理规范已经迫在眉睫，但仍然有很长的路要走。","link":"/2025/08/06/thoughts-AIGC/"},{"title":"Accelerating Your Program Through Coding Skills","text":"IntroductionWhen dealing with large-scale, complex optimization problems or training neural networks, we often encounter situations where programs run for extended periods or fail to complete. However, this is not necessarily due to the large problem scale or limitations of computer hardware capabilities. Even when attempting to use higher-performance servers or computers, there’s no guarantee of effectively accelerating code execution. This is because high-performance hardware typically needs to be matched with code designed for high-performance computing. This article aims to provide some code-level optimization strategies for program acceleration. By optimizing code structure and designing high-performance computing solutions, we can effectively accelerate program execution and improve runtime efficiency. It’s important to note that this article only covers code-level acceleration solutions and does not include optimization measures related to algorithms, hardware, etc. The article is written based on personal experience, and if there are any shortcomings, please point them out. Code OptimizationSimply put, there are mainly two approaches to implementing program optimization. One is to parallelize tasks and write parallel code to leverage the parallel computing capabilities of multi-core CPUs or GPUs to accelerate program execution. The other is to utilize compiler code optimization mechanisms to compile parts of code that require interpreters (such as Python, MATLAB) into machine code, achieving faster program execution speeds. 1 ParallelizationParallelization requires coordination between software and hardware, but the prerequisite is that the overall task can be decomposed into subtasks that can be executed simultaneously. There are two ways to achieve parallelization: one relies on multi-core CPUs to implement multi-process operations, and the other relies on GPUs. We will introduce the implementation methods of these two approaches below. 1.1 CPU Multi-process OperationsMulti-process operations leverage the multi-core characteristics of CPUs to achieve parallel computing. In multi-process operations, programs are decomposed into multiple subtasks, each running in an independent process. These processes can execute different tasks in parallel, thereby accelerating program execution. Multi-process operations can be implemented using Python’s multiprocessing library. When using the multiprocessing library for multi-process operations, it’s first necessary to decompose tasks into multiple subtasks and assign each subtask to different processes for execution. The following is a code framework for the multiprocessing library: 123456789101112131415161718import multiprocessingdef worker(num): print(f&quot;Worker {num} is running&quot;) returnif __name__ == &quot;__main__&quot;: processes = [] num_processes = 4 for i in range(num_processes): p = multiprocessing.Process(target=worker, args=(i,)) processes.append(p) p.start() for p in processes: p.join() print(&quot;All workers are done&quot;) In this example, we first define a worker function, which is the task that each subprocess will execute. Then in the main process (if name==”main“), we create num_processes subprocesses and add them to the processes list. Next, we iterate through the processes list, use the start() method from the Process class to start each subprocess, and wait for them to complete. Finally, we output “All workers are done” to indicate that all subprocesses have finished execution. 1.2 GPUCompared to CPUs, GPUs have more computing cores and higher computational power, making them better suited for parallel computing. Therefore, utilizing GPUs for code optimization can significantly improve program execution efficiency. To use GPUs for code optimization in Python, GPU programming frameworks such as NVIDIA’s CUDA framework are commonly used. 1.2.1 Python Implementation of GPU Parallel ComputingCUDA, developed by NVIDIA, is a GPU programming framework that allows developers to leverage the parallel computing capabilities of GPUs to accelerate various compute-intensive tasks. CUDA provides a set of APIs that facilitate GPU programming and supports multiple programming languages, including C/C++, Python, and Java. When using CUDA, the CUDA toolkit, which includes the CUDA driver, runtime library, and tools, is required. The following is a simple example of vector addition using the CUDA framework: 12345678910111213141516171819202122232425262728293031323334353637import numpy as npfrom numba import cuda# Define the vector addition function@cuda.jitdef vector_add(a, b, c): i = cuda.grid(1) if i &lt; len(c): c[i] = a[i] + b[i]# Define the main programif __name__ == '__main__': # Define the vector size n = 100000 # Generate random vectors on the host a = np.random.randn(n).astype(np.float32) b = np.random.randn(n).astype(np.float32) c = np.zeros(n, dtype=np.float32) # Transfer data to GPU memory d_a = cuda.to_device(a) d_b = cuda.to_device(b) d_c = cuda.to_device(c) # Define thread blocks and the number of threads threads_per_block = 64 blocks_per_grid = (n + (threads_per_block - 1)) // threads_per_block # Perform the vector addition operation vector_add[blocks_per_grid, threads_per_block](d_a, d_b, d_c) # Transfer the result from GPU memory back to host memory d_c.copy_to_host(c) # Print the result print(c) In the above code, we first define a vector_add function to add two vectors and store the result in a third vector. Then we generate two random vectors and transfer them to GPU memory. Next, we define thread blocks and the number of threads. In the GPU section, we need to pay attention to vectorization, i.e., writing code that utilizes matrix operations rather than serial computations with multiple for loops. 1.2.2 Using GPU to Train Neural NetworksWhen it comes to training deep neural networks, GPUs can demonstrate their advantages, as training neural networks often requires a large number of matrix operations, which is precisely the task GPUs excel at. Here is a simple example of neural network training code based on PyTorch and CUDA: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import torchimport torch.nn as nnimport torch.optim as optim# Define the neural network modelclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x# Define the training data and labelsinputs = torch.randn(1, 3, 32, 32)labels = torch.randn(1, 10)# Move the neural network model to GPUnet = Net().cuda()# Move the training data and labels to GPUinputs = inputs.cuda()labels = labels.cuda()# Define the loss function and optimizercriterion = nn.MSELoss()optimizer = optim.SGD(net.parameters(), lr=0.01)# Start trainingfor epoch in range(100): optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() print('Epoch %d, Loss: %.4f' % (epoch+1, loss.item())) In the above code, we use the xxx.cuda() method to load the model, training data, and labels into GPU memory, so that all computations involved in the training loop are carried out on the GPU. 1.2.3 PrecautionsIt’s particularly important to note that if you want to use a GPU for acceleration, you must ensure that the code is vectorized. Simply put, try to use matrix operations to represent the numerical computation process instead of using multiple for loops in a nested manner. This is because GPUs, compared to CPUs, are only stronger in terms of parallelism, but their computational power is inferior to that of CPUs. Serial computations like multiple for loops are not suitable for GPUs. To put it in the words of Dr. Li Mu, if a CPU is like a college student, a GPU is like a group of elementary school students. A college student can handle tasks like calculus, while elementary school students can only handle basic arithmetic. However, if a calculus task is broken down into multiple basic arithmetic operations, then the advantage of a group of elementary students, i.e., the GPU, becomes apparent. To give a simple example, suppose we want to design a loss function like this:$$L_\\theta=-\\sum_{i=1}^{N}\\sum_{j=1}^{N_i}w_{ij}log(p(x_i^j|M))$$If we don’t deliberately pay attention to the vectorization of the code, a common-sense approach might look like this: 123456789101112131415161718192021222324252627def lossFunc(y_pred,sols,objs):batch size=y_pred.shape[0]loss=torch.tensor(0.0)for i in range(batch_size): nSols=sols[i].shape[0] #Number of feasible solutions under the current batch (MIP) nVars=sols[i].shape[1] #Objective function normalization den=objs[i].sum() for l in range(objs[i].shape[0]): objs[i][l]=objs[i][l]/den den=sum(exp(-objs[i]))#Calculate the denominator for wii coefficient sum1=torch.tensor(0.0) for j in range(nSols): #Calculate weight wij w=exp(-objs[il[j])/den #Calculate the probability of feasible solution generation P=torch.tensor(1.9) for k in range(nVars): if sols[il[j,k]==1: P=p*y_pred[i][k] elif sols[il[j,k]==0: P=p*(1-y_pred[i][k]) #Calculate the summation sum1+=w*p loss+=sum1loss=-lossreturn loss But in reality, a loss function designed with multiple nested for loops is not suitable for GPU execution and may even perform worse than on a CPU. Test results showed that with such a loss function, one epoch would take about an hour…. making it impossible to train the neural network. 123456789101112131415161718192021222324def lossFunc(y,sols,objs): &quot;&quot;&quot; Loss function Parameters ---------- y : Neural network output batch_size x nVars sols : Set of feasible solutions batch_size x nSols x nVars objs : Objective function values corresponding to feasible solutions batch_size x nSols Returns ------- loss : Loss on the current batch &quot;&quot;&quot; objs=objs/15 eObjs=exp(-objs) den=eObjs.sum(axis=1) den=den.unsqueeze(1) w=eObjs/den y=y.unsqueeze(1) p=y*sols+(1-y)*(1-sols) p=log(p+1e-45) P=p.sum(axis=2) loss=-(w*P).sum() return loss So the correct approach should be like this, which not only shows significant acceleration on the GPU but also looks much cleaner… The downside is that you have to constantly use the broadcasting mechanism of matrix operations and it’s best to hand-calculate a few times with small-scale examples, calculating while designing. 2 Compiler Acceleration2.1 PrinciplesIn computer programming, compiled languages and interpreted languages are two common types. Compared to interpreted languages, compiled languages execute faster because they need to compile the code into executable binary code before execution. This is because compilers can optimize the source code into more efficient machine code, thereby speeding up program execution. There are many ways for compiler optimization, among which the most common include: Eliminating unnecessary calculations: Compilers can identify unnecessary calculations during code compilation, avoiding waste of computational resources. Loop unrolling: Loop unrolling refers to the practice of repeating the code in the loop body several times to reduce the number of loop iterations. This can improve the program’s running speed. Matrix/Vectorization: Matrix/Vectorization refers to placing multiple data into a matrix or vector and then performing calculations all at once. This can reduce the number of loop iterations and thus improve the program’s running speed. To help developers conveniently utilize compiler optimization for code, some open-source JIT compilers like Numba have been developed. These compilers can convert Python and other interpreted language codes into executable machine code, thus improving program execution speed. 2.2 Python Acceleration Solution Based on NumbaNumba is an open-source JIT compiler that can convert Python code into machine code, achieving code acceleration. Numba supports various optimization techniques, including loop unrolling, code vectorization, etc. Using Numba can greatly increase the execution speed of Python code. The following is a general code framework for using Numba to achieve Python code acceleration: Import the numba library Define a function that needs optimization Use the @numba.jit decorator to decorate the function, generating the numba-optimized function Call the optimized function To illustrate, let’s consider a comparative case that involves a deeply nested for loop to compute the determinant of a matrix. This is a compute-intensive operation that can benefit from acceleration using Numba. Original Python code: 12345678910111213141516171819def det(matrix): n = len(matrix) if n == 1: return matrix[0][0] elif n == 2: return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0] else: result = 0 for j in range(n): sub_matrix = [] for i in range(1, n): row = [] for k in range(n): if k != j: row.append(matrix[i][k]) sub_matrix.append(row) result += matrix[0][j] * det(sub_matrix) * (-1) ** j return result As we can see, this function contains deeply nested for loops, which are severely limited by the performance of the Python interpreter. Now let’s use Numba to accelerate it. Optimized Numba code: 1234567891011121314151617181920import numba@numba.jit(nopython=True)def det(matrix): n = len(matrix) if n == 1: return matrix[0][0] elif n == 2: return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0] else: result = 0 for j in range(n): sub_matrix = np.zeros((n-1, n-1)) for i in range(1, n): for k in range(n): if k != j: sub_matrix[i-1, k-(k&gt;j)] = matrix[i, k] result += matrix[0][j] * det(sub_matrix) * (-1) ** j return result Here, we use the @numba.jit(nopython=True) decorator to declare the function as one that can be accelerated by Numba. Simultaneously, we replace Python lists with Numpy arrays and use Numpy array slicing and broadcasting features to reduce loops and memory allocation. Testing code: 1234567891011121314151617import timeimport numpy as np# Generate a random 10x10 matrixmatrix = np.random.rand(10, 10)# Time the original Python codestart = time.time()d = det(matrix)end = time.time()print(f&quot;Python code took {end-start:.4f} seconds, result={d}&quot;)# Time the Numba-optimized codestart = time.time()d = det(matrix)end = time.time()print(f&quot;Numba-optimized code took {end-start:.4f} seconds, result={d}&quot;) Testing results: 12Python code took 0.5960 seconds, result=-0.004127521725273144Numba-optimized code took 0.0040 seconds, result=-0.004127521725273144 It’s important to note that Numba is not almighty, and there are certain limitations to the types of code it can accelerate. If the function decorated with the decorator is nested with many functions from third-party libraries, Numba may not be able to work.","link":"/2023/03/06/techArt-Code_acc/"}],"tags":[],"categories":[{"name":"Technical Articles","slug":"Technical-Articles","link":"/categories/Technical-Articles/"},{"name":"Thoughts","slug":"Thoughts","link":"/categories/Thoughts/"}],"pages":[{"title":"Chuanqing Pu","text":".project-card-vertical { position: relative; } /* 轮播外层：裁剪溢出，避免下方空白 */ .slider { position: relative; width: 100%; overflow: hidden; } /* 轨道：用 transform 横移 */ .slider-track { display: flex; transition: transform 0.4s ease; will-change: transform; } /* 每页宽度固定为 100% 视口 */ .slide { min-width: 100%; margin: 0; /* 覆盖主题对 figure 的默认 margin */ } /* 图片自适应 */ .slide img { display: block; width: 100%; height: auto; border-radius: 8px; } /* 说明文字 */ .slide figcaption { font-size: 0.9rem; text-align: center; margin-top: 6px; color: #666; } /* 左右按钮 */ .slider-btn { position: absolute; top: 50%; transform: translateY(-50%); z-index: 5; border: none; background: rgba(0,0,0,0.4); color: #fff; width: 36px; height: 36px; border-radius: 999px; cursor: pointer; line-height: 36px; text-align: center; font-size: 20px; user-select: none; } .slider-btn.prev { left: 8px; } .slider-btn.next { right: 8px; } .slider-btn:focus { outline: 2px solid #88aaff; } /* 保险：彻底清掉主题可能加在 figure 上的额外间距 */ figure.slide { margin: 0 !important; } (function () { function setupSlider(slider) { const track = slider.querySelector('.slider-track'); const slides = slider.querySelectorAll('.slide'); const prevBtn = slider.querySelector('.slider-btn.prev'); const nextBtn = slider.querySelector('.slider-btn.next'); if (!track || slides.length === 0) return; let index = 0; function update() { track.style.transform = `translateX(-${index * 100}%)`; // 可选：禁用到头的按钮 if (prevBtn) prevBtn.disabled = (index === 0); if (nextBtn) nextBtn.disabled = (index === slides.length - 1); } function go(step) { index = Math.max(0, Math.min(slides.length - 1, index + step)); update(); } prevBtn && prevBtn.addEventListener('click', () => go(-1)); nextBtn && nextBtn.addEventListener('click', () => go(1)); // 键盘支持 slider.setAttribute('tabindex', '0'); slider.addEventListener('keydown', (e) => { if (e.key === 'ArrowLeft') { e.preventDefault(); go(-1); } if (e.key === 'ArrowRight') { e.preventDefault(); go(1); } }); // 简单触摸滑动 let startX = 0, dx = 0, touching = false; const threshold = 40; slider.addEventListener('touchstart', (e) => { touching = true; startX = e.touches[0].clientX; dx = 0; }, {passive:true}); slider.addEventListener('touchmove', (e) => { if (!touching) return; dx = e.touches[0].clientX - startX; }, {passive:true}); slider.addEventListener('touchend', () => { if (!touching) return; if (dx > threshold) go(-1); else if (dx < -threshold) go(1); touching = false; dx = 0; }); // 初始同步 update(); } function initAll() { document.querySelectorAll('.slider').forEach(setupSlider); } // 确保在 DOM 就绪后执行（适配 Hexo 主题是否把脚本放在 head） if (document.readyState === 'loading') { document.addEventListener('DOMContentLoaded', initAll); } else { initAll(); } // 适配 Icarus/Next 等 PJAX 主题 document.addEventListener('pjax:complete', initAll); })(); PhD Candidate, Electrical EngineeringShanghai Jiao Tong University Email: sashabanks@sjtu.edu.cn ·GitHub: https://github.com/BigdogManLuo Research InterestsOptimization of Energy Systems · Learning to Optimize (L2O) · Predict-then-Optimize · Time Series Forecasting · Decision-Focused Learning BiographyI’m currently an explorer of Electrical Engineering and Data Science. Research/Engineering Experience Online Aggregated Wind-Solar Probabilistic Forecasting and Trading in GB Electricity Market (1st student team in HEFTCom) Period: 2023.12-2024.12 We proposed a comprehensive solution for the IEEE Hybrid Energy Forecasting and Trading Competition . The solution provides accurate probabilistic forecasts for a wind–solar hybrid system and achieves substantial trading revenue in the day-ahead electricity market. Key components include: (1) a stacking-based approach that combines sister forecasts from multiple Numerical Weather Predictions (NWPs) to generate wind power forecasts, (2) an online solar post-processing model designed to mitigate the distribution shift in the online test set caused by increased solar capacity, (3) a probabilistic aggregation method for generating accurate quantile forecasts of hybrid generation, and (4) a stochastic trading strategy that maximizes expected trading revenue under uncertain electricity prices. Following the competition, we prepared a detailed description of our solution and conducted additional case studies to evaluate the proposed methods beyond the competition setting. The code for all the methods is publicly available to facilitate reproduction and to support further research in both academia and industry. Keywords: Wind power forecasting, Solar power forecasting, Electricity price forecasting, Probabilistic forecasting, Renewable Energy Trading, Probabilistic aggregation Paper · Code ‹ Workflow overview Result › Education Ph.D. in Electrical Engineering, Shanghai Jiao Tong University, 2023 – PresentResearch Interest: Learning-based Optimization for Energy Systems; Applications of Decision-focused Learning in Energy Systems B.Eng. in Electrical Engineering, Sichuan University, 2019 – 2023 Awards &amp; Honors1st place (student team) in the IEEE Hybrid Energy Forecasting and Trading CompetitionAs the leader of the team “GEB”, I participate in the IEEE Hybrid Energy Forecasting and Trading Competition (HEFTCom24) and achieve 3rd/4th in the trading and forecasting track. We are also award as the “best placed student team” in HEFTCom24 and are invited to participate in the ISF2024 to report our methodology. Our solutions provide accurate probabilistic forecasts for a hybrid power plant and achieving significant trading revenue in the day-ahead electricity market. (2023.12-2024.07) Published codebase for reproducibility. Research paper can be found at arxiv Grand Pirce (1st) in 2025 THS Forecasting HackathonThis is a 7-hour on-site programming competition to predict quarterly tourist arrivals from the top-5 source markets to Hong Kong and Macau for 2023–2025. I won the championship with the lowest MAE. (2025.07) Codebase: comming soon National 1st Prize of “TI Cup” National Undergraduate Electronic Design CompetitionDesign of control algorithms for three-phase inverter and three-phase PFC rectifier (based on C language and DSP unit). These control algorithms include Butterworth filter, three-phase/single-phase phase-locked loop, PID controller, proportional resonant (PR) controller, and 32/dq conversion unit. (2020.09-2021.12) For details, see: Competition Codebase PCB Source File Report Vedio Disclaimer: This hardware kit is provided for educational and experimental use only. The author assumes no responsibility for any injury, damage, or loss resulting from its use. Best paper award in 2023 PandaFPEWe propose a two-stage coordination framework for wind farms with energy storage systems. The framework integrates day-ahead dynamic programming and real-time rolling optimization to maximize joint energy and frequency regulation revenues, while accounting for prediction errors and internal power distribution. (2023.05) Codebase SkillsOptimization Modeling PyTorchCVXPY Time SeriesADMMEnergy SystemsMIP / MINLP","link":"/about/index.html"}]}