

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/blog/img/fluid.png">
  <link rel="icon" href="/blog/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="BigdogManLuo">
  <meta name="keywords" content="">
  
    <meta name="description" content="IntroductionWhen dealing with large-scale, complex optimization problems or training neural networks, we often encounter situations where programs run for extended periods or fail to complete. However">
<meta property="og:type" content="article">
<meta property="og:title" content="Accelerating Your Program Through Coding Skills">
<meta property="og:url" content="https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/index.html">
<meta property="og:site_name" content="Try to Understand">
<meta property="og:description" content="IntroductionWhen dealing with large-scale, complex optimization problems or training neural networks, we often encounter situations where programs run for extended periods or fail to complete. However">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-03-05T16:00:00.000Z">
<meta property="article:modified_time" content="2025-07-19T15:21:49.760Z">
<meta property="article:author" content="BigdogManLuo">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>Accelerating Your Program Through Coding Skills - Try to Understand</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/blog/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/blog/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/blog/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"bigdogmanluo.github.io","root":"/blog/","version":"1.9.8","typing":{"enable":true,"typeSpeed":30,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/blog/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/blog/js/utils.js" ></script>
  <script  src="/blog/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/blog/">
      <strong>Try to Understand</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/blog/img/bg.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Accelerating Your Program Through Coding Skills"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-03-06 00:00" pubdate>
          March 6, 2023 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          869 words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          8 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Accelerating Your Program Through Coding Skills</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>When dealing with large-scale, complex optimization problems or training neural networks, we often encounter situations where programs run for extended periods or fail to complete. However, this is not necessarily due to the large problem scale or limitations of computer hardware capabilities. Even when attempting to use higher-performance servers or computers, there’s no guarantee of effectively accelerating code execution. This is because high-performance hardware typically needs to be matched with code designed for high-performance computing.</p>
<p>This article aims to provide some code-level optimization strategies for program acceleration. By optimizing code structure and designing high-performance computing solutions, we can effectively accelerate program execution and improve runtime efficiency. It’s important to note that this article only covers code-level acceleration solutions and does not include optimization measures related to algorithms, hardware, etc. The article is written based on personal experience, and if there are any shortcomings, please point them out.</p>
<h1 id="Code-Optimization"><a href="#Code-Optimization" class="headerlink" title="Code Optimization"></a>Code Optimization</h1><p>Simply put, there are mainly two approaches to implementing program optimization. One is to parallelize tasks and write parallel code to leverage the parallel computing capabilities of multi-core CPUs or GPUs to accelerate program execution. The other is to utilize compiler code optimization mechanisms to compile parts of code that require interpreters (such as Python, MATLAB) into machine code, achieving faster program execution speeds.</p>
<h2 id="1-Parallelization"><a href="#1-Parallelization" class="headerlink" title="1 Parallelization"></a>1 Parallelization</h2><p>Parallelization requires coordination between software and hardware, but the prerequisite is that the overall task can be decomposed into subtasks that can be executed simultaneously. There are two ways to achieve parallelization: one relies on multi-core CPUs to implement multi-process operations, and the other relies on GPUs. We will introduce the implementation methods of these two approaches below.</p>
<h3 id="1-1-CPU-Multi-process-Operations"><a href="#1-1-CPU-Multi-process-Operations" class="headerlink" title="1.1 CPU Multi-process Operations"></a>1.1 CPU Multi-process Operations</h3><p>Multi-process operations leverage the multi-core characteristics of CPUs to achieve parallel computing. In multi-process operations, programs are decomposed into multiple subtasks, each running in an independent process. These processes can execute different tasks in parallel, thereby accelerating program execution. Multi-process operations can be implemented using Python’s multiprocessing library.</p>
<p>When using the multiprocessing library for multi-process operations, it’s first necessary to decompose tasks into multiple subtasks and assign each subtask to different processes for execution.</p>
<p>The following is a code framework for the multiprocessing library:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> multiprocessing<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">worker</span>(<span class="hljs-params">num</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Worker <span class="hljs-subst">&#123;num&#125;</span> is running&quot;</span>)<br>    <span class="hljs-keyword">return</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    processes = []<br>    num_processes = <span class="hljs-number">4</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_processes):<br>        p = multiprocessing.Process(target=worker, args=(i,))<br>        processes.append(p)<br>        p.start()<br><br>    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> processes:<br>        p.join()<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;All workers are done&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>In this example, we first define a <code>worker</code> function, which is the task that each subprocess will execute. Then in the main process (if <strong>name</strong>&#x3D;&#x3D;”<strong>main</strong>“), we create <code>num_processes</code> subprocesses and add them to the <code>processes</code> list. Next, we iterate through the <code>processes</code> list, use the start() method from the Process class to start each subprocess, and wait for them to complete. Finally, we output “All workers are done” to indicate that all subprocesses have finished execution.</p>
<h3 id="1-2-GPU"><a href="#1-2-GPU" class="headerlink" title="1.2 GPU"></a>1.2 GPU</h3><p>Compared to CPUs, GPUs have more computing cores and higher computational power, making them better suited for parallel computing. Therefore, utilizing GPUs for code optimization can significantly improve program execution efficiency. To use GPUs for code optimization in Python, GPU programming frameworks such as NVIDIA’s CUDA framework are commonly used.</p>
<h4 id="1-2-1-Python-Implementation-of-GPU-Parallel-Computing"><a href="#1-2-1-Python-Implementation-of-GPU-Parallel-Computing" class="headerlink" title="1.2.1 Python Implementation of GPU Parallel Computing"></a>1.2.1 Python Implementation of GPU Parallel Computing</h4><p>CUDA, developed by NVIDIA, is a GPU programming framework that allows developers to leverage the parallel computing capabilities of GPUs to accelerate various compute-intensive tasks. CUDA provides a set of APIs that facilitate GPU programming and supports multiple programming languages, including C&#x2F;C++, Python, and Java. When using CUDA, the CUDA toolkit, which includes the CUDA driver, runtime library, and tools, is required.</p>
<p>The following is a simple example of vector addition using the CUDA framework:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> numba <span class="hljs-keyword">import</span> cuda<br><br><span class="hljs-comment"># Define the vector addition function</span><br><span class="hljs-meta">@cuda.jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vector_add</span>(<span class="hljs-params">a, b, c</span>):<br>    i = cuda.grid(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-built_in">len</span>(c):<br>        c[i] = a[i] + b[i]<br><br><span class="hljs-comment"># Define the main program</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># Define the vector size</span><br>    n = <span class="hljs-number">100000</span><br><br>    <span class="hljs-comment"># Generate random vectors on the host</span><br>    a = np.random.randn(n).astype(np.float32)<br>    b = np.random.randn(n).astype(np.float32)<br>    c = np.zeros(n, dtype=np.float32)<br><br>    <span class="hljs-comment"># Transfer data to GPU memory</span><br>    d_a = cuda.to_device(a)<br>    d_b = cuda.to_device(b)<br>    d_c = cuda.to_device(c)<br><br>    <span class="hljs-comment"># Define thread blocks and the number of threads</span><br>    threads_per_block = <span class="hljs-number">64</span><br>    blocks_per_grid = (n + (threads_per_block - <span class="hljs-number">1</span>)) // threads_per_block<br><br>    <span class="hljs-comment"># Perform the vector addition operation</span><br>    vector_add[blocks_per_grid, threads_per_block](d_a, d_b, d_c)<br><br>    <span class="hljs-comment"># Transfer the result from GPU memory back to host memory</span><br>    d_c.copy_to_host(c)<br><br>    <span class="hljs-comment"># Print the result</span><br>    <span class="hljs-built_in">print</span>(c)<br></code></pre></td></tr></table></figure>
<p>In the above code, we first define a <code>vector_add</code> function to add two vectors and store the result in a third vector. Then we generate two random vectors and transfer them to GPU memory. Next, we define thread blocks and the number of threads. In the GPU section, we need to pay attention to vectorization, i.e., writing code that utilizes matrix operations rather than serial computations with multiple for loops.</p>
<h4 id="1-2-2-Using-GPU-to-Train-Neural-Networks"><a href="#1-2-2-Using-GPU-to-Train-Neural-Networks" class="headerlink" title="1.2.2 Using GPU to Train Neural Networks"></a>1.2.2 Using GPU to Train Neural Networks</h4><p>When it comes to training deep neural networks, GPUs can demonstrate their advantages, as training neural networks often requires a large number of matrix operations, which is precisely the task GPUs excel at.</p>
<p>Here is a simple example of neural network training code based on PyTorch and CUDA:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br><span class="hljs-comment"># Define the neural network model</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Net, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.pool = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv1(x)))<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv2(x)))<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>)<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># Define the training data and labels</span><br>inputs = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)<br>labels = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># Move the neural network model to GPU</span><br>net = Net().cuda()<br><br><span class="hljs-comment"># Move the training data and labels to GPU</span><br>inputs = inputs.cuda()<br>labels = labels.cuda()<br><br><span class="hljs-comment"># Define the loss function and optimizer</span><br>criterion = nn.MSELoss()<br>optimizer = optim.SGD(net.parameters(), lr=<span class="hljs-number">0.01</span>)<br><br><span class="hljs-comment"># Start training</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    optimizer.zero_grad()<br>    outputs = net(inputs)<br>    loss = criterion(outputs, labels)<br>    loss.backward()<br>    optimizer.step()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch %d, Loss: %.4f&#x27;</span> % (epoch+<span class="hljs-number">1</span>, loss.item()))<br><br></code></pre></td></tr></table></figure>

<p>In the above code, we use the <code>xxx.cuda()</code> method to load the model, training data, and labels into GPU memory, so that all computations involved in the training loop are carried out on the GPU.</p>
<h4 id="1-2-3-Precautions"><a href="#1-2-3-Precautions" class="headerlink" title="1.2.3 Precautions"></a>1.2.3 Precautions</h4><p>It’s particularly important to note that if you want to use a GPU for acceleration, you must ensure that the code is vectorized. Simply put, try to use matrix operations to represent the numerical computation process instead of using multiple for loops in a nested manner.</p>
<p>This is because GPUs, compared to CPUs, are only stronger in terms of parallelism, but their computational power is inferior to that of CPUs. Serial computations like multiple for loops are not suitable for GPUs. To put it in the words of Dr. Li Mu, if a CPU is like a college student, a GPU is like a group of elementary school students. A college student can handle tasks like calculus, while elementary school students can only handle basic arithmetic. However, if a calculus task is broken down into multiple basic arithmetic operations, then the advantage of a group of elementary students, i.e., the GPU, becomes apparent.</p>
<p>To give a simple example, suppose we want to design a loss function like this:<br>$$L_\theta&#x3D;-\sum_{i&#x3D;1}^{N}\sum_{j&#x3D;1}^{N_i}w_{ij}log(p(x_i^j|M))$$<br>If we don’t deliberately pay attention to the vectorization of the code, a common-sense approach might look like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lossFunc</span>(<span class="hljs-params">y_pred,sols,objs</span>):<br>batch size=y_pred.shape[<span class="hljs-number">0</span>]<br>loss=torch.tensor(<span class="hljs-number">0.0</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>	nSols=sols[i].shape[<span class="hljs-number">0</span>] <span class="hljs-comment">#Number of feasible solutions under the current batch (MIP)</span><br>	nVars=sols[i].shape[<span class="hljs-number">1</span>]<br>	<span class="hljs-comment">#Objective function normalization</span><br>	den=objs[i].<span class="hljs-built_in">sum</span>()<br>	<span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(objs[i].shape[<span class="hljs-number">0</span>]):<br>		objs[i][l]=objs[i][l]/den<br>	den=<span class="hljs-built_in">sum</span>(exp(-objs[i]))<span class="hljs-comment">#Calculate the denominator for wii coefficient</span><br>	sum1=torch.tensor(<span class="hljs-number">0.0</span>)<br>	<span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(nSols):<br>		<span class="hljs-comment">#Calculate weight wij</span><br>		w=exp(-objs[il[j])/den<br>		<span class="hljs-comment">#Calculate the probability of feasible solution generation</span><br>		P=torch.tensor(<span class="hljs-number">1.9</span>)<br>		<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(nVars):<br>			<span class="hljs-keyword">if</span> sols[il[j,k]==<span class="hljs-number">1</span>:<br>				P=p*y_pred[i][k]<br>			<span class="hljs-keyword">elif</span> sols[il[j,k]==<span class="hljs-number">0</span>:<br>				P=p*(<span class="hljs-number">1</span>-y_pred[i][k])<br>		<span class="hljs-comment">#Calculate the summation</span><br>		sum1+=w*p<br>	loss+=sum1<br>loss=-loss<br><span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure>
<p>But in reality, a loss function designed with multiple nested for loops is not suitable for GPU execution and may even perform worse than on a CPU. Test results showed that with such a loss function, one epoch would take about an hour…. making it impossible to train the neural network.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lossFunc</span>(<span class="hljs-params">y,sols,objs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Loss function</span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    y : Neural network output batch_size x nVars</span><br><span class="hljs-string">    sols : Set of feasible solutions batch_size x nSols x nVars</span><br><span class="hljs-string">    objs : Objective function values corresponding to feasible solutions batch_size x nSols</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    loss : Loss on the current batch</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    objs=objs/<span class="hljs-number">15</span><br>    eObjs=exp(-objs)<br>    den=eObjs.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>)<br>    den=den.unsqueeze(<span class="hljs-number">1</span>)<br>    w=eObjs/den<br>    y=y.unsqueeze(<span class="hljs-number">1</span>)<br>    p=y*sols+(<span class="hljs-number">1</span>-y)*(<span class="hljs-number">1</span>-sols)<br>    p=log(p+<span class="hljs-number">1e-45</span>)<br>    P=p.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">2</span>)<br>    loss=-(w*P).<span class="hljs-built_in">sum</span>()<br>    <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure>
<p>So the correct approach should be like this, which not only shows significant acceleration on the GPU but also looks much cleaner… The downside is that you have to constantly use the broadcasting mechanism of matrix operations and it’s best to hand-calculate a few times with small-scale examples, calculating while designing.</p>
<h2 id="2-Compiler-Acceleration"><a href="#2-Compiler-Acceleration" class="headerlink" title="2 Compiler Acceleration"></a>2 Compiler Acceleration</h2><h3 id="2-1-Principles"><a href="#2-1-Principles" class="headerlink" title="2.1 Principles"></a>2.1 Principles</h3><p>In computer programming, compiled languages and interpreted languages are two common types. Compared to interpreted languages, compiled languages execute faster because they need to compile the code into executable binary code before execution. This is because compilers can optimize the source code into more efficient machine code, thereby speeding up program execution.</p>
<p>There are many ways for compiler optimization, among which the most common include:</p>
<ol>
<li><p>Eliminating unnecessary calculations: Compilers can identify unnecessary calculations during code compilation, avoiding waste of computational resources.</p>
</li>
<li><p>Loop unrolling: Loop unrolling refers to the practice of repeating the code in the loop body several times to reduce the number of loop iterations. This can improve the program’s running speed.</p>
</li>
<li><p>Matrix&#x2F;Vectorization: Matrix&#x2F;Vectorization refers to placing multiple data into a matrix or vector and then performing calculations all at once. This can reduce the number of loop iterations and thus improve the program’s running speed.</p>
</li>
</ol>
<p>To help developers conveniently utilize compiler optimization for code, some open-source JIT compilers like Numba have been developed. These compilers can convert Python and other interpreted language codes into executable machine code, thus improving program execution speed.</p>
<h3 id="2-2-Python-Acceleration-Solution-Based-on-Numba"><a href="#2-2-Python-Acceleration-Solution-Based-on-Numba" class="headerlink" title="2.2 Python Acceleration Solution Based on Numba"></a>2.2 Python Acceleration Solution Based on Numba</h3><p>Numba is an open-source JIT compiler that can convert Python code into machine code, achieving code acceleration. Numba supports various optimization techniques, including loop unrolling, code vectorization, etc. Using Numba can greatly increase the execution speed of Python code.</p>
<p>The following is a general code framework for using Numba to achieve Python code acceleration:</p>
<ol>
<li>Import the numba library</li>
<li>Define a function that needs optimization</li>
<li>Use the @numba.jit decorator to decorate the function, generating the numba-optimized function</li>
<li>Call the optimized function</li>
</ol>
<p>To illustrate, let’s consider a comparative case that involves a deeply nested for loop to compute the determinant of a matrix. This is a compute-intensive operation that can benefit from acceleration using Numba.</p>
<p>Original Python code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">det</span>(<span class="hljs-params">matrix</span>):<br>    n = <span class="hljs-built_in">len</span>(matrix)<br>    <span class="hljs-keyword">if</span> n == <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> matrix[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">elif</span> n == <span class="hljs-number">2</span>:<br>        <span class="hljs-keyword">return</span> matrix[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] * matrix[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] - matrix[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] * matrix[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">else</span>:<br>        result = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            sub_matrix = []<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n):<br>                row = []<br>                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>                    <span class="hljs-keyword">if</span> k != j:<br>                        row.append(matrix[i][k])<br>                sub_matrix.append(row)<br>            result += matrix[<span class="hljs-number">0</span>][j] * det(sub_matrix) * (-<span class="hljs-number">1</span>) ** j<br>        <span class="hljs-keyword">return</span> result<br><br></code></pre></td></tr></table></figure>
<p>As we can see, this function contains deeply nested for loops, which are severely limited by the performance of the Python interpreter. Now let’s use Numba to accelerate it.</p>
<p>Optimized Numba code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numba<br><br><span class="hljs-meta">@numba.jit(<span class="hljs-params">nopython=<span class="hljs-literal">True</span></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">det</span>(<span class="hljs-params">matrix</span>):<br>    n = <span class="hljs-built_in">len</span>(matrix)<br>    <span class="hljs-keyword">if</span> n == <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> matrix[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">elif</span> n == <span class="hljs-number">2</span>:<br>        <span class="hljs-keyword">return</span> matrix[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] * matrix[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] - matrix[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] * matrix[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">else</span>:<br>        result = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            sub_matrix = np.zeros((n-<span class="hljs-number">1</span>, n-<span class="hljs-number">1</span>))<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n):<br>                <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>                    <span class="hljs-keyword">if</span> k != j:<br>                        sub_matrix[i-<span class="hljs-number">1</span>, k-(k&gt;j)] = matrix[i, k]<br>            result += matrix[<span class="hljs-number">0</span>][j] * det(sub_matrix) * (-<span class="hljs-number">1</span>) ** j<br>        <span class="hljs-keyword">return</span> result<br><br></code></pre></td></tr></table></figure>
<p>Here, we use the <code>@numba.jit(nopython=True)</code> decorator to declare the function as one that can be accelerated by Numba. Simultaneously, we replace Python lists with Numpy arrays and use Numpy array slicing and broadcasting features to reduce loops and memory allocation.</p>
<p>Testing code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Generate a random 10x10 matrix</span><br>matrix = np.random.rand(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># Time the original Python code</span><br>start = time.time()<br>d = det(matrix)<br>end = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Python code took <span class="hljs-subst">&#123;end-start:<span class="hljs-number">.4</span>f&#125;</span> seconds, result=<span class="hljs-subst">&#123;d&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># Time the Numba-optimized code</span><br>start = time.time()<br>d = det(matrix)<br>end = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Numba-optimized code took <span class="hljs-subst">&#123;end-start:<span class="hljs-number">.4</span>f&#125;</span> seconds, result=<span class="hljs-subst">&#123;d&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>Testing results:</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs css">Python <span class="hljs-selector-tag">code</span> took <span class="hljs-number">0.5960</span> seconds, result=-<span class="hljs-number">0.004127521725273144</span><br>Numba-optimized <span class="hljs-selector-tag">code</span> took <span class="hljs-number">0.0040</span> seconds, result=-<span class="hljs-number">0.004127521725273144</span><br></code></pre></td></tr></table></figure>


<p>It’s important to note that Numba is not almighty, and there are certain limitations to the types of code it can accelerate. If the function decorated with the decorator is nested with many functions from third-party libraries, Numba may not be able to work.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/blog/categories/Technical-Articles/" class="category-chain-item">Technical Articles</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Accelerating Your Program Through Coding Skills</div>
      <div>https://bigdogmanluo.github.io/2023/03/06/techArt-Code_acc/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>BigdogManLuo</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>March 6, 2023</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/blog/2025/07/08/thoughts-creativity/" title="The Creativity">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">The Creativity</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/blog/2022/04/15/techArt-AutoEmail/" title="裁缝式开发：用MATLAB批量发送一封图文并茂的邮件">
                        <span class="hidden-mobile">裁缝式开发：用MATLAB批量发送一封图文并茂的邮件</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/blog/js/events.js" ></script>
<script  src="/blog/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/blog/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/blog/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/blog/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
